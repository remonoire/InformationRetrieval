{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingsWrapper():\n",
    "    \"\"\"\n",
    "    This postings wrapper creates a link between the index dictionary and the postings list.\n",
    "    \"\"\"\n",
    "    def __init__(self, postings_list, posting, postings_index):\n",
    "        self.frequency = 1\n",
    "        self.postings_index = postings_index\n",
    "        postings_list.append([posting])\n",
    "        \n",
    "\n",
    "    def add_posting(self, postings_list, posting):\n",
    "        \"\"\"\n",
    "        \n",
    "        Adds a posting to the postings list, at correct index according to the term\n",
    "        Only called if the term has yet not corresponding postings.\n",
    "        \n",
    "        :param postings_list: postings list, an attribute of the index.\n",
    "        :param posting: the posting to be added, extracted from a list of tokens and docids.\n",
    "        :return: returns nothing\n",
    "        \"\"\"\n",
    "        if posting not in postings_list[self.postings_index]:\n",
    "            postings_list[self.postings_index].append(posting)\n",
    "            self.frequency += 1\n",
    "\n",
    "de_dic = []\n",
    "with open('german.dic', 'r', encoding='latin-1') as f:\n",
    "    for row in f:\n",
    "        if len(row) >1:\n",
    "            de_dic.append(row.strip().lower())\n",
    "en_dic = []\n",
    "with open('english.dic', 'r',) as f:\n",
    "    for row in f:\n",
    "        en_dic.append(row.strip().lower())\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "\n",
    "class index:\n",
    "    \"\"\"\n",
    "    Processes the tweets.csv file or any file containing the same structure, creates\n",
    "    an inverted index. This is a dictionary terms as keys and an instance of the PostingsWrapper \n",
    "    class as value. Also creates a seperate postings list, also as an attribute, which contains\n",
    "    all tweet ids where each term occured.\n",
    "    \"\"\"\n",
    "    def __init__(self, file):\n",
    "        \"\"\"\n",
    "        :param file: path to tweets.csv file.\n",
    "        \"\"\"\n",
    "        self.data, self.data_index, self.data_dic = self.preprocess(file)\n",
    "        self.index, self.postings_list = self.create_index()\n",
    "        self.top_de, self.top_en = self.top_freq() \n",
    "\n",
    "    def preprocess(self, file):\n",
    "        \"\"\"\n",
    "        Opens raw text, spits it into lines comprised of six columns, stores in intermediary\n",
    "        tab_seperated variable.\n",
    "        Then proceeds to normalize this while transfering it to data variable. Everything is lowered\n",
    "        and compared to a regex which desires to only extract usernames and tokens containing \n",
    "        only letters. All irrelevant columns are disgarded.\n",
    "        \n",
    "        :param file: path to tweets.csv file.\n",
    "        :return: data, containing tweet IDs with corresponding tweets \n",
    "        and a dictionary of all terms and original text\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import string\n",
    "        \n",
    "        raw_text = open(file).read()\n",
    "        tab_seperated = [item.split('\\t') for item in raw_text.split('\\n')]\n",
    "\n",
    "        for line in tab_seperated:\n",
    "            if len(line) == 1:\n",
    "                tab_seperated.remove(line)\n",
    "\n",
    "        data = []\n",
    "        data_index = {}\n",
    "        for i in range(len(tab_seperated)):\n",
    "            data.append([tab_seperated[i][1], tab_seperated[i][4].lower()])\n",
    "            data_index[tab_seperated[i][1]] = tab_seperated[i][4]\n",
    "            \n",
    "        data = data\n",
    "        contractions = [\"it's\", \"he's\",\"she's\",\"that's\", \"what's\", \"there's\",\\\n",
    "                        \"[newline]\", \"'m\", \"'ve\",\"n't\", \"'ll\",\"'re\", \"won't\", \"'d\", \"'s\"]\n",
    "        fixes = [\"it is\", \"he is\",\"she is\",\"that is\", \"what is\", \"there is\",\\\n",
    "                 \" \", \" am\", \" have\", \" not\", \" will\", \" are\", \"will not\", \" would\", \"\"]\n",
    "        for line in data:\n",
    "            for i in range(len(contractions)):\n",
    "                if contractions[i] in line[1]:\n",
    "                    line[1]= line[1].replace(contractions[i], fixes[i])\n",
    "            line[1] = re.sub(r'[^\\w\\s]', ' ' , line[1])\n",
    "            line[1] = re.sub(r'[0-9].*\\s', ' ' , line[1])\n",
    "            line[1] = re.sub(r'https?.+\\s', ' ' , line[1])\n",
    "            line[1] = re.sub(r'[\\W].+[^\\W\\s]+|[^ ]+\\.[^ ]+ |[^a-zA-Zäöüß\\s]+ \\\n",
    "                             | \\d+|[^\\w\\s]+.[^\\W\\s]+| https?','', line[1])\n",
    "            #line[1] = re.sub('https?:\\/\\/[^\\s]*|[^a-z\\s]', '', line[1])\n",
    "        data_dic = {}\n",
    "        for row in data:\n",
    "            data_dic[row[0]] = row[1]\n",
    "\n",
    "        return data, data_index, data_dic\n",
    "\n",
    "    def create_index(self):\n",
    "        \"\"\"\n",
    "        Creates the index and postings list.\n",
    "        :return: index, a dictionary having a unique term as key and a PostingsWrapper instance\n",
    "        as value, and postings_list, a large list of lists containing all postings for each unique\n",
    "        term.\n",
    "        \"\"\"\n",
    "\n",
    "        # We initialize the index, the postings list, and an intermediary tokens_and_ids variable.\n",
    "        index = {}\n",
    "        postings_list = []\n",
    "        tokens_and_ids = []\n",
    "\n",
    "        # For each line in data, we split each tweet by whitespace into tokens.\n",
    "        # As a simple preprocessing step we check to make sure that the length of each token is\n",
    "        # > 0 before appending the token and its tweet ID to the tokens_and_ids list.\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_en = set(stopwords.words('english'))\n",
    "        stop_germ = set(stopwords.words('german'))\n",
    "        \n",
    "        for line in self.data:\n",
    "            for token in [x for x in line[1].split()  if (not x in stop_en) and (not x in stop_germ)]:\n",
    "                if len(token) > 1:\n",
    "                    tokens_and_ids.append([token, line[0]])\n",
    "\n",
    "        # We sort our list of all tokens.\n",
    "        \n",
    "        tokens_and_ids.sort()\n",
    "\n",
    "        # The postings_index variable we initialize here will be used as we instantiate\n",
    "        # PostingsWrapper objects. This integer will enable us to keep track of the index\n",
    "        # of the postings list where all of a given term's postings are contained.\n",
    "        \n",
    "        postings_index = 0\n",
    "        \n",
    "        # For each line in tokens_and_ids, we check to make sure it is not already in our index.\n",
    "        # If it is not we add it, create a corresponding PostingsWrapper Object that will\n",
    "        # add to the postings list as it is initialized. The PostingsWrapper will also keep track\n",
    "        # of frequency for us.\n",
    "        # Having done this we then increment the postings_index variable by 1.\n",
    "        # If it is found that the term is already present in our index, we simply add the new \n",
    "        # posting to its postings list using the PostingsWrapper.add_posting method.\n",
    "        for line in tokens_and_ids:\n",
    "            if line[0] not in index.keys():\n",
    "                index[line[0]] = PostingsWrapper(postings_list, line[1], postings_index)\n",
    "                postings_index += 1\n",
    "            else:\n",
    "                 index[line[0]].add_posting(postings_list, line[1])\n",
    "\n",
    "        return index, postings_list\n",
    "            \n",
    "    def get_frequency(self, term):\n",
    "        \"\"\"\n",
    "        Pulls frequency from wrapper of term\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return index.index[term].frequency\n",
    "        except:\n",
    "            print('Term not found.')\n",
    "    \n",
    "    def All_frequencies(self):\n",
    "        '''\n",
    "        return the term and frequencies in descending order\n",
    "        '''\n",
    "        frequencies = []\n",
    "        for term in self.index.keys():\n",
    "            frequencies.append((self.index[term].frequency, term))\n",
    "        return sorted(frequencies)[::-1]\n",
    "    def top_freq(self):\n",
    "        freq_de = []\n",
    "        freq_en = []\n",
    "        frequencies = self.All_frequencies()[:200]\n",
    "        for i,j in frequencies:\n",
    "            if self.is_language(j) == 'german':\n",
    "                freq_de.append(j)\n",
    "            else:\n",
    "                freq_en.append(j)\n",
    "        return freq_de, freq_en\n",
    "    \n",
    "    def is_language(self,term):\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "        stop_en = set(stopwords.words('english'))\n",
    "        stop_germ = set(stopwords.words('german'))\n",
    "        \n",
    "        g_score = 0\n",
    "        en_score = 0\n",
    "        for post in self.postings_list[self.index[term].postings_index]:\n",
    "            for i in self.data_index[post].lower().strip().split():\n",
    "                if i in stop_germ:\n",
    "                    g_score += 1\n",
    "                if i in stop_en:\n",
    "                    en_score += 1\n",
    "#         print('DE:', g_score,'\\t','EN:',en_score)\n",
    "        if g_score>en_score:\n",
    "            return 'german'\n",
    "#             print('probability:', (g_score/(g_score+en_score)))\n",
    "#             print(\"german\")\n",
    "        elif g_score<en_score:\n",
    "            return 'english'\n",
    "#             print('probability:', (en_score/(g_score+en_score)))\n",
    "#             print('english')\n",
    "        else:\n",
    "            return 'unsure'\n",
    "    \n",
    "    def language(self,post):\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "        contractions = [\"it's\", \"he's\",\"she's\",\"that's\", \"what's\", \"there's\",\\\n",
    "                        \"[newline]\", \"'m\", \"'ve\",\"n't\", \"'ll\",\"'re\", \"won't\", \"'d\", \"'s\"]\n",
    "        fixes = [\"it is\", \"he is\",\"she is\",\"that is\", \"what is\", \"there is\",\\\n",
    "                 \" \", \" am\", \" have\", \" not\", \" will\", \" are\", \"will not\", \" would\", \"\"]\n",
    "        stop_en = set(stopwords.words('english'))\n",
    "        stop_germ = set(stopwords.words('german'))\n",
    "        de_char = ['ä','ö','ü','ß']\n",
    "        g_score = 0\n",
    "        en_score = 0\n",
    "        \n",
    "        for char in self.data_index[post].lower():\n",
    "            if char in de_char:\n",
    "                g_score +=1\n",
    "\n",
    "        text = self.data_index[post].lower()\n",
    "        for x in range(len(contractions)):\n",
    "            text = text.replace(contractions[x], fixes[x])\n",
    "        text = re.sub(r'[^\\w\\s]','', text)\n",
    "        for i in text.strip().split():    \n",
    "            if i in stop_germ or i in self.top_de:\n",
    "                g_score += 1\n",
    "            if i in stop_en or i in self.top_en:\n",
    "                en_score += 1\n",
    "        #print(post, 'DE:', g_score,'\\t','EN:',en_score)\n",
    "        if g_score>en_score:\n",
    "            return 'german'\n",
    "#             print('probability:', (g_score/(g_score+en_score)))\n",
    "#             print(\"german\")\n",
    "        elif en_score>g_score:\n",
    "            return 'english'\n",
    "#             print('probability:', (en_score/(g_score+en_score)))\n",
    "#             print('english')\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_misspells(self):\n",
    "        #from nltk.stem import GermanWortschatzLemmatizer as gwl\n",
    "        terms = sorted(self.index.keys())\n",
    "        de = []\n",
    "        en = []\n",
    "        for x in terms[:200]:\n",
    "            term = ''\n",
    "            for i in range(len(x)):\n",
    "                try:\n",
    "                    if x[i] != [i+2]:\n",
    "                        term += x[i]\n",
    "                except:\n",
    "                    term += x[i]\n",
    "            g_count = 0\n",
    "            e_count = 0\n",
    "            e_posts = []\n",
    "            g_posts = []\n",
    "            tags = ['n','v','a','s','r']\n",
    "            en_lemma = []\n",
    "            for tag in tags:\n",
    "                en_lemma.append(wnl.lemmatize(wnl,word=term, pos=tag))\n",
    "            en_lemma = set(en_lemma)\n",
    "            de_lemma = term\n",
    "            for post in self.postings_list[self.index[term].postings_index]:\n",
    "                if self.language(post) == 'german':\n",
    "                    ## Apply Lematizer to word here\n",
    "                    ## get german misspelling count\n",
    "                    if de_lemma not in de_dic:\n",
    "#                         for i in en_lemma:\n",
    "#                             if i in en_dic:\n",
    "#                                 continue    \n",
    "                        g_count += 1\n",
    "                        g_posts.append(post)\n",
    "                elif self.language(post) == 'english':\n",
    "                    ## Apply Lematizer to word here\n",
    "                    ## get german misspelling count\n",
    "                    if len([i for i in en_lemma if i in en_dic]) == 0:\n",
    "#                     if de_lemma not in de_dic:\n",
    "                        e_count += 1\n",
    "                        e_posts.append(post)\n",
    "                else:\n",
    "                    lang = self.is_language(term)\n",
    "                    if lang == None:\n",
    "                        continue\n",
    "                    if lang == 'german':\n",
    "                        ## Apply Lematizer to word here\n",
    "                        ## get german misspelling count    \n",
    "                        if de_lemma not in de_dic:\n",
    "#                             for i in en_lemma:\n",
    "#                                 if i in en_dic:\n",
    "#                                     continue\n",
    "                            g_count += 1\n",
    "                            g_posts.append(post)\n",
    "                    else:\n",
    "                        ## Apply Lematizer to word here\n",
    "                        ## get german misspelling count\n",
    "                        if len([i for i in en_lemma if i in en_dic]) == 0:\n",
    "#                             if de_lemma not in de_dic:\n",
    "                            e_count += 1\n",
    "                            e_posts.append(post)\n",
    "                            \n",
    "            if g_count > 0:\n",
    "                de.append((g_count, term, g_posts))\n",
    "            if e_count > 0:\n",
    "                en.append((e_count, term, e_posts))\n",
    "        return sorted(de)[::-1], sorted(en)[::-1]\n",
    "    \n",
    "    def en_damerau(self, term):\n",
    "        alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        term = term.lower()\n",
    "        splits     = [(term[:i], term[i:])    for i in range(len(term) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in alphabet]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in alphabet]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "    \n",
    "    def de_damerau(self, term):\n",
    "        alphabet = \"abcdefghijklmnopqrstuvwxyzäöüß\"\n",
    "        term = term.lower()\n",
    "        splits     = [(term[:i], term[i:])    for i in range(len(term) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in alphabet]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in alphabet]\n",
    "        return set(deletes + transposes + replaces + inserts)    \n",
    "\n",
    "    def en_suggested(self,term):\n",
    "        suggestions = self.en_damerau(term)\n",
    "        suggested = []\n",
    "        for i in suggestions:\n",
    "            tags = ['n','v','a','s','r']\n",
    "            lemmas = []\n",
    "            for tag in tags:\n",
    "                lemmas.append(wnl.lemmatize(wnl,word=i, pos=tag))\n",
    "            lemmas = set(lemmas)\n",
    "            for j in lemmas:\n",
    "                if j in en_dic:\n",
    "                    suggested.append(i)\n",
    "        best = []\n",
    "        count = 0\n",
    "        for k in set(suggested):\n",
    "            counter = 0\n",
    "            for h in k:\n",
    "                if h in k:\n",
    "                    counter +=1\n",
    "            if counter >= count and counter < len(i):\n",
    "                count = counter\n",
    "                best.append(k)\n",
    "        return best\n",
    "#             if i in en_dic:\n",
    "#                 suggested.append(i)\n",
    "#         return suggested\n",
    "    \n",
    "    def de_suggested(self,term):\n",
    "        suggestions = self.de_damerau(term)\n",
    "        suggested = []\n",
    "        for i in suggestions:\n",
    "            if i in de_dic:\n",
    "                suggested.append(i)\n",
    "        return suggested\n",
    "        \n",
    "    def spell_check(self, lst):\n",
    "        de_dic = []\n",
    "        with open('german.dic', 'r', encoding='latin-1') as f:\n",
    "            for row in f:\n",
    "                if len(row) >1:\n",
    "                    de_dic.append(row.strip().lower())\n",
    "        en_dic = []\n",
    "        with open('english.dic', 'r',) as f:\n",
    "            for row in f:\n",
    "                en_dic.append(row.strip().lower())\n",
    "        for post in lst:\n",
    "            if self.language(post) == 'german':\n",
    "                print('German')\n",
    "                misspells = [i for i in self.data_dic[post].split() if i not in de_dic] \n",
    "            else:\n",
    "                print('English')\n",
    "                misspells = [i for i in self.data_dic[post].split() if i not in en_dic]\n",
    "            print(\"Number of misspellings:\", len(misspells))\n",
    "            print(misspells)\n",
    "        return\n",
    "    \n",
    "    def query_one(self, term):\n",
    "        \"\"\"\n",
    "        Queries for a term.\n",
    "        :param term: query term\n",
    "        :return: postings list corresponding to query term, or error message if no results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.postings_list[index.index[term].postings_index]\n",
    "        except:\n",
    "            print('No results for query.')\n",
    "#         try:\n",
    "#             for posting in self.postings_list[index.index[term].postings_index]:\n",
    "#                 print(posting, self.data_index[posting], '\\n')\n",
    "#         except:\n",
    "#             print('No results for query.')\n",
    "\n",
    "\n",
    "    def query_and(self, term1, term2):\n",
    "        \"\"\"\n",
    "        Queries for the intersection of two terms.\n",
    "        :param term1: first term\n",
    "        :param term2: second term\n",
    "        :return: returns intersection of postings lists of both terms.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Here we compare the two lists and create iterators to help us compare the two postings lists\n",
    "        def And(post1,post2):\n",
    "            if len(post1) < len(post2):    \n",
    "                iterpost1 = iter(post1)\n",
    "                iterpost2 = iter(post2)\n",
    "            else:\n",
    "                iterpost1 = iter(post2)\n",
    "                iterpost2 = iter(post1)\n",
    "                \n",
    "        # Here we initialize an empty intersection variable which will (hopefully) be filled.\n",
    "            intersection = []\n",
    "            \n",
    "            current1 = next(iterpost1)\n",
    "            current2 = next(iterpost2)\n",
    "        # This is the loop that iterates over the members of each postings list, comparing them.\n",
    "        # If there is a match it will be added to the intersection.\n",
    "            while True:\n",
    "                if current1 == current2:\n",
    "                    intersection.append(current1)\n",
    "                    try:\n",
    "                        current1 = next(iterpost1)\n",
    "                        current2 = next(iterpost2)\n",
    "                    except:\n",
    "                        break\n",
    "                elif current1 < current2:\n",
    "                    try:\n",
    "                        current1 = next(iterpost1)\n",
    "                    except:\n",
    "                        break\n",
    "                else:\n",
    "                    try:\n",
    "                        current2 = next(iterpost2)\n",
    "                    except:\n",
    "                        break\n",
    "            # Here we print each text and id number found in intersection\n",
    "#             if len(intersection) != 0:\n",
    "#                 for i in intersection:\n",
    "#                     print( i, self.data_index[i], '\\n')\n",
    "                    \n",
    "#             else:\n",
    "#                 print('No results for query.')\n",
    "            # Here we access the postings list for each term, assign them to variables.\n",
    "#         if type(term1) == list:\n",
    "#             try:\n",
    "#                 postings2 = self.postings_list[index.index[term2].postings_index]\n",
    "#                 return And(term1, postings2)\n",
    "#             except:\n",
    "#                 return None\n",
    "        try:\n",
    "            postings1 = self.postings_list[index.index[term1].postings_index]\n",
    "            postings2 = self.postings_list[index.index[term2].postings_index]\n",
    "            return And(postings1, postings2)\n",
    "        except:\n",
    "            print('Error: 1 or more terms not found.')\n",
    "#     def query_three(self, term1, term2, term3):\n",
    "#         self.query_and(self.query_and(term1,term2),term3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "renou_index = index('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('renou_index5.pkl', 'wb') as f:\n",
    "    pickle.dump(renou_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('renou_index5.pkl', 'rb') as f:\n",
    "    index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frog\n"
     ]
    }
   ],
   "source": [
    "a = 'frog'\n",
    "new_a = ''\n",
    "for i in range(len(a)):\n",
    "    try:\n",
    "        if a[i] != a[i+2]:\n",
    "            new_a += a[i]\n",
    "    except:\n",
    "        new_a += a[i]\n",
    "print(new_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_miss, en_miss = index.get_misspells()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#de_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'german'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.language('1001528572438249472')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Acun \"Alp atamıyor\" diyemez ki...Biliyor çünkü Alp onun sesini ayırt edeceğini. Kıyamaz ki #survivor2018 #hilmur https://t.co/FOLgUgSGR5'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.data_index['980858193700995072']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12,\n",
       "  'adhd',\n",
       "  ['1022985712432566272',\n",
       "   '959830428550074368',\n",
       "   '963160559159046145',\n",
       "   '966066703577726976',\n",
       "   '966380202002604032',\n",
       "   '966449273633112070',\n",
       "   '973254925269553152',\n",
       "   '979830050273902592',\n",
       "   '983421236444368896',\n",
       "   '983836480123486209',\n",
       "   '985643221874339840',\n",
       "   '987415731779375104']),\n",
       " (10,\n",
       "  'ahhh',\n",
       "  ['1002329630118903808',\n",
       "   '1005361121199607809',\n",
       "   '1007170534033981440',\n",
       "   '990008920566648832',\n",
       "   '991373407487975424',\n",
       "   '995451547873173504',\n",
       "   '995758846219051010',\n",
       "   '997920017340649474',\n",
       "   '997926777036201984',\n",
       "   '997964501587722240']),\n",
       " (6,\n",
       "  'ahh',\n",
       "  ['1005131932416598016',\n",
       "   '1017539553207472128',\n",
       "   '994303222549549057',\n",
       "   '996394375998558208',\n",
       "   '998397003053838342',\n",
       "   '999776002355224577']),\n",
       " (4,\n",
       "  'abschlussplakat',\n",
       "  ['1003297214318465024',\n",
       "   '1003373516488626176',\n",
       "   '1008591002729435138',\n",
       "   '1009414480076435456']),\n",
       " (3,\n",
       "  'ahs',\n",
       "  ['976916199249534977', '976916207814217730', '976934206478708736']),\n",
       " (3,\n",
       "  'aafa',\n",
       "  ['984473698303344640', '984473701860151297', '984473728410116097']),\n",
       " (2, 'ahhhh', ['967512821453017095', '990519904502796289']),\n",
       " (2, 'afrin', ['978685890611511296', '983377037451845632']),\n",
       " (2, 'abiball', ['1011236381950140416', '1011341487634894848']),\n",
       " (2, 'abc', ['973714846607859713', '978011598462713856']),\n",
       " (2, 'aaaand', ['1024057512092463104', '997161424454483968']),\n",
       " (1, 'akehh', ['989288161221201921']),\n",
       " (1, 'ahhiutfqsl', ['979939420135739392']),\n",
       " (1, 'ahhhhhhhhhh', ['1008112748323069952']),\n",
       " (1, 'ahflcir', ['1002607726197919744']),\n",
       " (1, 'ahdhfj', ['1024363169681207296']),\n",
       " (1, 'ahahhhahhahah', ['995427832506482688']),\n",
       " (1, 'agr', ['983686378578997249']),\n",
       " (1, 'aghsj', ['989596598689779713']),\n",
       " (1, 'aghdjagd', ['1007297707483033601']),\n",
       " (1, 'afl', ['1012328104977674240']),\n",
       " (1, 'afk', ['1014269555819106311']),\n",
       " (1, 'affender', ['989506459330211840']),\n",
       " (1, 'afa', ['1010606419555618816']),\n",
       " (1, 'aerosmith', ['982295005179105287']),\n",
       " (1, 'adı', ['967168711948931072']),\n",
       " (1, 'advertisments', ['1016099079980503040']),\n",
       " (1, 'adsgdjsj', ['991291979228155905']),\n",
       " (1, 'adoro', ['966521483781689350']),\n",
       " (1, 'adorf', ['1012753350474059776']),\n",
       " (1, 'admin', ['971288570609721344']),\n",
       " (1, 'adeta', ['981471275792166912']),\n",
       " (1, 'adderall', ['990733491468939264']),\n",
       " (1, 'activa', ['971173098304299009']),\n",
       " (1, 'acissej', ['1012123313659883525']),\n",
       " (1, 'acidobacteria', ['966997050154995717']),\n",
       " (1, 'acia', ['992888368462639104']),\n",
       " (1, 'achtung', ['1017429993134182401']),\n",
       " (1, 'achtelfinale', ['1013523793879412737']),\n",
       " (1, 'acho', ['967121568253038595']),\n",
       " (1, 'acetaminophen', ['988875579578568704']),\n",
       " (1, 'acabo', ['963788873141772288']),\n",
       " (1, 'abteilungsleiter', ['990916758809796608']),\n",
       " (1, 'abschlussfeier', ['1021035464348745729']),\n",
       " (1, 'ablenkung', ['980930675980292102']),\n",
       " (1, 'abi', ['1016039677596459010']),\n",
       " (1, 'abendstimmung', ['1023556459534135296']),\n",
       " (1, 'abendspaziergang', ['994589601951092736']),\n",
       " (1, 'abendromantik', ['989449289943437315']),\n",
       " (1, 'abcam', ['1013742875484545024']),\n",
       " (1, 'abca', ['1010169215972765702']),\n",
       " (1, 'aatrox', ['1006886540142874624']),\n",
       " (1, 'aarburg', ['997242321786757120']),\n",
       " (1, 'aangenaam', ['966664210531930112']),\n",
       " (1, 'aah', ['998566478755573761']),\n",
       " (1, 'aacr', ['981333909546811392']),\n",
       " (1, 'aachen', ['998302129097007105']),\n",
       " (1, 'aaahhhhh', ['989467399916326912']),\n",
       " (1, 'aaaahhaha', ['998988652834377728']),\n",
       " (1, 'aaaaaah', ['1017072593097158656']),\n",
       " (1, 'aaaaa', ['995394515220549633'])]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'german'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.language('1002324883651678208')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Actually kann ich jeden verstehen der mit League aufgehört habe..[NEWLINE]Wenn in der CS gesagt wird, dass deine Mutter hoffentlich einen Tumor hat dann weiß ich wirklich nicht was los ist.[NEWLINE]@RiotSupport @riotgames Wieso ist das Reportsystem einfach aus 2001?'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.data_index['1002324883651678208']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dah',\n",
       " 'wah',\n",
       " 'aal',\n",
       " 'auh',\n",
       " 'bah',\n",
       " 'mah',\n",
       " 'aas',\n",
       " 'aam',\n",
       " 'ash',\n",
       " 'pah',\n",
       " 'sah',\n",
       " 'ach',\n",
       " 'rah',\n",
       " 'aha',\n",
       " 'hah',\n",
       " 'yah']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.en_suggested('aah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'activin' in en_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer as wnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerates\n",
      "accelerate\n",
      "accelerates\n",
      "accelerates\n",
      "accelerates\n"
     ]
    }
   ],
   "source": [
    "tags = ['n','v','r','a','s']\n",
    "for tag in tags:\n",
    "    print(wnl.lemmatize(wnl,word='accelerates', pos=tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = index.All_frequencies()[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_de = []\n",
    "freq_en = []\n",
    "for i,j in test:\n",
    "    if index.is_language(j) == 'german':\n",
    "        freq_de.append(j)\n",
    "    else:\n",
    "        freq_en.append(j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10003"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.de_suggested('agenturleute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
