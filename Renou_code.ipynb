{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingsWrapper():\n",
    "    \"\"\"\n",
    "    This postings wrapper creates a link between the index dictionary and the postings list.\n",
    "    \"\"\"\n",
    "    def __init__(self, postings_list, posting, postings_index):\n",
    "        self.frequency = 1\n",
    "        self.postings_index = postings_index\n",
    "        postings_list.append([posting])\n",
    "        \n",
    "\n",
    "    def add_posting(self, postings_list, posting):\n",
    "        \"\"\"\n",
    "        \n",
    "        Adds a posting to the postings list, at correct index according to the term\n",
    "        Only called if the term has yet not corresponding postings.\n",
    "        \n",
    "        :param postings_list: postings list, an attribute of the index.\n",
    "        :param posting: the posting to be added, extracted from a list of tokens and docids.\n",
    "        :return: returns nothing\n",
    "        \"\"\"\n",
    "        if posting not in postings_list[self.postings_index]:\n",
    "            postings_list[self.postings_index].append(posting)\n",
    "            self.frequency += 1\n",
    "\n",
    "\n",
    "class index:\n",
    "    \"\"\"\n",
    "    Processes the tweets.csv file or any file containing the same structure, creates\n",
    "    an inverted index. This is a dictionary terms as keys and an instance of the PostingsWrapper \n",
    "    class as value. Also creates a seperate postings list, also as an attribute, which contains\n",
    "    all tweet ids where each term occured.\n",
    "    \"\"\"\n",
    "    def __init__(self, file):\n",
    "        \"\"\"\n",
    "        :param file: path to tweets.csv file.\n",
    "        \"\"\"\n",
    "        self.data, self.data_index = self.preprocess(file)\n",
    "        self.index, self.postings_list = self.create_index()\n",
    "\n",
    "    def preprocess(self, file):\n",
    "        \"\"\"\n",
    "        Opens raw text, spits it into lines comprised of six columns, stores in intermediary\n",
    "        tab_seperated variable.\n",
    "        Then proceeds to normalize this while transfering it to data variable. Everything is lowered\n",
    "        and compared to a regex which desires to only extract usernames and tokens containing \n",
    "        only letters. All irrelevant columns are disgarded.\n",
    "        \n",
    "        :param file: path to tweets.csv file.\n",
    "        :return: data, containing tweet IDs with corresponding tweets \n",
    "        and a dictionary of all terms and original text\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        raw_text = open(file).read()\n",
    "        tab_seperated = [item.split('\\t') for item in raw_text.split('\\n')]\n",
    "\n",
    "        for line in tab_seperated:\n",
    "            if len(line) == 1:\n",
    "                tab_seperated.remove(line)\n",
    "\n",
    "        data = []\n",
    "        data_index = {}\n",
    "        for i in range(len(tab_seperated)):\n",
    "            data.append([tab_seperated[i][1], tab_seperated[i][4].lower()])\n",
    "            data_index[tab_seperated[i][1]] = tab_seperated[i][4]\n",
    "            \n",
    "        data = data\n",
    "\n",
    "        for line in data:\n",
    "            line[1] = line[1].replace('newline',' ')\n",
    "            line[1] = re.sub(r'[^\\w\\s]+.[^\\W\\s]+|[^ ]+\\.[^ ]+ |[^a-zA-ZÃ¤Ã¶Ã¼ÃŸ\\s]+ | \\d+|[^\\w\\s]+.[^\\W\\s]+| https?','', line[1])\n",
    "            #line[1] = re.sub('https?:\\/\\/[^\\s]*|[^a-z\\s]', '', line[1])\n",
    "\n",
    "        return data, data_index\n",
    "\n",
    "    def create_index(self):\n",
    "        \"\"\"\n",
    "        Creates the index and postings list.\n",
    "        :return: index, a dictionary having a unique term as key and a PostingsWrapper instance\n",
    "        as value, and postings_list, a large list of lists containing all postings for each unique\n",
    "        term.\n",
    "        \"\"\"\n",
    "\n",
    "        # We initialize the index, the postings list, and an intermediary tokens_and_ids variable.\n",
    "        index = {}\n",
    "        postings_list = []\n",
    "        tokens_and_ids = []\n",
    "\n",
    "        # For each line in data, we split each tweet by whitespace into tokens.\n",
    "        # As a simple preprocessing step we check to make sure that the length of each token is\n",
    "        # > 0 before appending the token and its tweet ID to the tokens_and_ids list.\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words2 = set(stopwords.words('german'))\n",
    "        \n",
    "        for line in self.data:\n",
    "            for token in [x for x in line[1].split()  if (not x in stop_words) and (not x in stop_words2)]:\n",
    "                if len(token) > 1:\n",
    "                    tokens_and_ids.append([token, line[0]])\n",
    "\n",
    "        # We sort our list of all tokens.\n",
    "        \n",
    "        tokens_and_ids.sort()\n",
    "\n",
    "        # The postings_index variable we initialize here will be used as we instantiate\n",
    "        # PostingsWrapper objects. This integer will enable us to keep track of the index\n",
    "        # of the postings list where all of a given term's postings are contained.\n",
    "        \n",
    "        postings_index = 0\n",
    "        \n",
    "        # For each line in tokens_and_ids, we check to make sure it is not already in our index.\n",
    "        # If it is not we add it, create a corresponding PostingsWrapper Object that will\n",
    "        # add to the postings list as it is initialized. The PostingsWrapper will also keep track\n",
    "        # of frequency for us.\n",
    "        # Having done this we then increment the postings_index variable by 1.\n",
    "        # If it is found that the term is already present in our index, we simply add the new \n",
    "        # posting to its postings list using the PostingsWrapper.add_posting method.\n",
    "        for line in tokens_and_ids:\n",
    "            if line[0] not in index.keys():\n",
    "                index[line[0]] = PostingsWrapper(postings_list, line[1], postings_index)\n",
    "                postings_index += 1\n",
    "            else:\n",
    "                index[line[0]].add_posting(postings_list, line[1])\n",
    "\n",
    "        return index, postings_list\n",
    "            \n",
    "    def get_frequency(self, term):\n",
    "        \"\"\"\n",
    "        Pulls frequency from wrapper of term\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return index.index[term].frequency\n",
    "        except:\n",
    "            print('Term not found.')\n",
    "    \n",
    "    def All_frequencies(self):\n",
    "        '''\n",
    "        return the term and frequencies in descending order\n",
    "        '''\n",
    "        frequencies = []\n",
    "        for term in index.index.keys():\n",
    "            frequencies.append((index.index[term].frequency, term))\n",
    "        return sorted(frequencies)[::-1]\n",
    "        \n",
    "    def query_one(self, term):\n",
    "        \"\"\"\n",
    "        Queries for a term.\n",
    "        :param term: query term\n",
    "        :return: postings list corresponding to query term, or error message if no results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for posting in self.postings_list[index.index[term].postings_index]:\n",
    "                print(posting, self.data_index[posting], '\\n')\n",
    "        except:\n",
    "            print('No results for query.')\n",
    "\n",
    "    def query_and(self, term1, term2):\n",
    "        \"\"\"\n",
    "        Queries for the intersection of two terms.\n",
    "        :param term1: first term\n",
    "        :param term2: second term\n",
    "        :return: returns intersection of postings lists of both terms.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Here we compare the two lists and create iterators to help us compare the two postings lists\n",
    "        def And(post1,post2):\n",
    "            if len(post1) < len(post2):    \n",
    "                iterpost1 = iter(post1)\n",
    "                iterpost2 = iter(post2)\n",
    "            else:\n",
    "                iterpost1 = iter(post2)\n",
    "                iterpost2 = iter(post1)\n",
    "                \n",
    "        # Here we initialize an empty intersection variable which will (hopefully) be filled.\n",
    "            intersection = []\n",
    "            \n",
    "            current1 = next(iterpost1)\n",
    "            current2 = next(iterpost2)\n",
    "        # This is the loop that iterates over the members of each postings list, comparing them.\n",
    "        # If there is a match it will be added to the intersection.\n",
    "            while True:\n",
    "                if current1 == current2:\n",
    "                    intersection.append(current1)\n",
    "                    try:\n",
    "                        current1 = next(iterpost1)\n",
    "                        current2 = next(iterpost2)\n",
    "                    except:\n",
    "                        break\n",
    "                elif current1 < current2:\n",
    "                    try:\n",
    "                        current1 = next(iterpost1)\n",
    "                    except:\n",
    "                        break\n",
    "                else:\n",
    "                    try:\n",
    "                        current2 = next(iterpost2)\n",
    "                    except:\n",
    "                        break\n",
    "            # Here we print each text and id number found in intersection\n",
    "            if len(intersection) != 0:\n",
    "                for i in intersection:\n",
    "                    print( i, self.data_index[i], '\\n')\n",
    "                    \n",
    "            else:\n",
    "                print('No results for query.')\n",
    "            # Here we access the postings list for each term, assign them to variables.\n",
    "#         if type(term1) == list:\n",
    "#             try:\n",
    "#                 postings2 = self.postings_list[index.index[term2].postings_index]\n",
    "#                 return And(term1, postings2)\n",
    "#             except:\n",
    "#                 return None\n",
    "        try:\n",
    "            postings1 = self.postings_list[index.index[term1].postings_index]\n",
    "            postings2 = self.postings_list[index.index[term2].postings_index]\n",
    "            return And(postings1, postings2)\n",
    "        except:\n",
    "            print('Error: 1 or more terms not found.')\n",
    "#     def query_three(self, term1, term2, term3):\n",
    "#         self.query_and(self.query_and(term1,term2),term3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "renou_index = index('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('renou_index3.pkl', 'wb') as f:\n",
    "    pickle.dump(renou_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('renou_index3.pkl', 'rb') as f:\n",
    "    index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003063075333922821 einfach befriedigend mitten in der nacht aus dem offenen fenster zu sehen und zu wissen dass gerade alle schlafen, als wÃ¤r man der einzige mensch den es gerade gibt \n",
      "\n",
      "1009499399972642816 @the_necrosis @robin_urban Mein Betreuer wusste grob wegen #CRD und #non24h Bescheid. Wann waren die Termine mit ihm? SpÃ¤ter vormittag. Das hat mich so gestresst, dass ich die Nacht vorher noch spÃ¤ter erst schlafen konnte als eh schon. (War ja ohne #orphanmedi)[NEWLINE]Auch sicher keine #Inklusion. \n",
      "\n",
      "1011758043841916930 ich kann die letzten tage wieder besser schlafen und es ist so entlastend, mal mehr als vier stunden pro nacht zu schlafen obwohl man eigentlich ausschlafen kÃ¶nnte \n",
      "\n",
      "1016073619791925250 Gute Nacht Leute ich geh dann mal SchlafenðŸ‘‹ðŸ»[NEWLINE]#Faultier #Love #JaDasBinIch #BinIchNichtHÃ¼bsch https://t.co/uFX9Qwwoxu \n",
      "\n",
      "960632815414075392 @despacitolea @Polwnn @SofiStorm22 Ich werd am Valentinstag schlafen weil ich die Nacht durchschneiden muss #goals #love \n",
      "\n",
      "962473245520941056 Als ob er schlafen gegangen ist ohne gute Nacht oder so zu schreiben :( \n",
      "\n",
      "963523476685578240 Eine wilde Nacht in den Bergen der Heimat - schon -32 Grad auf der Alp HintergrÃ¤ppelen im Toggenburg und -36 Grad auf der Glattalp https://t.co/ClAp1BrM88 #uhuerechalt - da wird der @Kryophil heute Nacht nicht schlafen #Schweiz #wetter \n",
      "\n",
      "974795567711875072 @twittonium @Fotobiene Fotobiene ist gern ein bisschen experimentell polemisch, um zu schauen, was als nÃ¤chstes passiert ðŸ˜„[NEWLINE][NEWLINE]Und ich gehe jetzt schlafen![NEWLINE]Gute Nacht allerseits âœ¨ðŸŒ™ \n",
      "\n",
      "975515873875300354 @VergessenesHerz ..dir sanft Ã¼ber die haut streichle mich enger an dich schmiege und dir sanft durch die haare streichle und dich auf die stirn kÃ¼sse als du eingeschlafen bist.. werd hier bleiben und die nacht auf dich aufpassen damit du gut schlafen kannst :* \n",
      "\n",
      "976608314153930752 Ist ja nicht so, als hÃ¤tte ich es gewusst... Keine Ahnung wie vielte Nacht seit Wochen, aber ich kann nicht schlafen. HeiÃŸt morgen wieder schÃ¶n mÃ¼de in den Tag. Ich freue mich jetzt schon. https://t.co/iI1yBXACYA \n",
      "\n",
      "980231817860206592 Ich fersuche jetzt zu schlafen ðŸ’¤ðŸ˜´ ob ich nicht mÃ¼de ðŸ˜‘ bin aber ich habe Kopf schmerzen als so fersuche ich es jetzt. Gute Nacht Leute. ðŸ’¤ðŸ˜´ https://t.co/ck52dG9ipk \n",
      "\n",
      "983848050232905728 Ich glaub heute Nacht kann ich das mit dem schlafen Knicken. Habe fÃ¼r die 18+ BeitrÃ¤ge recherchiert und schon mehr echte Morde gesehen als ich eigentlich will. https://t.co/orrU5crTWp \n",
      "\n",
      "984686263054884864 Wie ich gerade gesehen habe, ist heute Freitag der 13. ErklÃ¤rt diesen angsteinflÃ¶ÃŸenden Horror, den ich heute Nacht getrÃ¤umt habe ðŸ˜± WÃ¼rde ich daraus ein Buch machen, kÃ¶nnten meine Leser nicht mehr ruhig schlafen. Mann, bin ich gerade verstÃ¶rt ... [NEWLINE][NEWLINE]#freitagder13te #Horror #Buch \n",
      "\n",
      "986386986343727105 Heute mal kein nachtgrind, will einfach mal etwas frÃ¼her schlafen gehen als sonst plus eklige kopfschmerzen... wÃ¼nsche euch allen trotzdessen eine gute nacht âœ¨â˜„ï¸ \n",
      "\n",
      "987118347815878657 @mavaho2 @blumedri @HuhnUschi @Lilly_HD_Neu @OliverSenst @zickenalarm1900 @ThomasRaue @webaxvita @danny_23032000 @Platoon100 Dann sind wir uns ja im groben einig und kÃ¶nnen schlafen gehen.Respektvoll schreiben war nett. Gute Nacht \n",
      "\n",
      "988552072009220096 So, ich werde mal schlafen gehen. Ich bin zwar immer noch in diesem beschissenen Tief und es geht mir nicht so gut, aber immerhin besser als gestern und der Tag war einigermaÃŸen gut. Danke fÃ¼r die Unterhaltungen heute. :) Ich wÃ¼nsche euch eine gute Nacht und schlaft schÃ¶n. \n",
      "\n",
      "989889332948426753 @Lassdichgrusche @GrillJimmy @_love_not_hate_ @DoggoNonGrata Dieser Love Overkill! Ich weiÃŸ nicht ob ich heute Nacht aus lauter Aufregung Ã¼berhaupt schlafen kann ðŸ˜… \n",
      "\n",
      "999002626955063296 Hab mir gerade son Blackhead-Removal auf Facebook angeschaut. Eventuell bin ich etwas traumatisiert. Ein war gewordener Albtraum, bei dem es aussieht als wÃ¼rde man ganz viele kleine dicke Maden ausm Gesicht drÃ¼cken. [NEWLINE][NEWLINE]Werde heute Nacht nicht schlafen kÃ¶nnen. \n",
      "\n",
      "999800701776736258 uff omg ich bin raus onkel klaus ich schau das morgen weiter wenn es hell ist gute nacht als ob ich schlafen kann fml \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#index.query_one('fr')\n",
    "#index.query_one('and')\n",
    "index.query_and('nacht', 'schlafen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5207, 'mehr'), (4493, 'happy'), (4225, 'mal'), (4164, 'really'), (3782, \"i'm\")]\n",
      "Term not found.\n"
     ]
    }
   ],
   "source": [
    "print(index.All_frequencies()[:5])\n",
    "index.get_frequency('fÃ¼r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
