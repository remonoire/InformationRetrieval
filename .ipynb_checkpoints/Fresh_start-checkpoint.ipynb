{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "from nltk.stem.snowball import GermanStemmer as gs\n",
    "from nltk.metrics.distance import edit_distance as lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = [\"it's\", \"he's\",\"she's\",\"that's\", \"what's\", \"there's\",\\\n",
    "                        \"[newline]\", \"'m\", \"'ve\",\"n't\", \"'ll\",\"'re\", \"won't\", \"'d\", \"'s\"]\n",
    "fixes = [\"it is\", \"he is\",\"she is\",\"that is\", \"what is\", \"there is\",\\\n",
    "                 \" \", \" am\", \" have\", \" not\", \" will\", \" are\", \"will not\", \" would\", \"\"]\n",
    "stop_en = set(stopwords.words('english'))\n",
    "stop_germ = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dic = {}\n",
    "with open('german.dic', 'r', encoding='latin-1') as f:\n",
    "    for row in f:\n",
    "        if len(row) >1:\n",
    "            de_dic[row.strip().lower()] = 1\n",
    "en_dic = {}\n",
    "with open('english.dic', 'r',) as f:\n",
    "    for row in f:\n",
    "        en_dic[row.strip().lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = open('tweets.csv').read()\n",
    "tab_seperated = [item.split('\\t') for item in raw_text.split('\\n') if len(item.split('\\t')) >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for i in range(len(tab_seperated)):\n",
    "    data[tab_seperated[i][1]] = tab_seperated[i][4]\n",
    "    \n",
    "data_index = data.copy()    \n",
    "for tweet in data_index.keys():\n",
    "    data_index[tweet] = data_index[tweet].lower()\n",
    "    for i in range(len(contractions)):\n",
    "        if contractions[i] in data_index[tweet]:\n",
    "            data_index[tweet] = data_index[tweet].replace(contractions[i], fixes[i])\n",
    "    data_index[tweet] = re.sub('https?[^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub('[@#][^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[0-9][^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'\\w+\\.[^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[^a-zäöüß\\s]', ' ', data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[^\\w\\s]', ' ' , data_index[tweet])\n",
    "    \n",
    "terms = {}\n",
    "term_index = {}\n",
    "for num,tweet in data_index.items():\n",
    "    for word in tweet.split():\n",
    "        if word in term_index:\n",
    "            term_index[word].append(num)\n",
    "        elif word not in term_index:\n",
    "            term_index[word] = [num]\n",
    "        if word in terms:\n",
    "            terms[word] += 1\n",
    "        else:\n",
    "            terms[word] = 1\n",
    "\n",
    "for key in term_index.keys():\n",
    "    term_index[key] = sorted(term_index[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_language(term):\n",
    "        de_char = ['ä','ö','ü','ß']\n",
    "        de_score = 0\n",
    "        en_score = 0\n",
    "        for post in term_index[term]:\n",
    "            for i in data_index[post].strip().split():\n",
    "                for char in de_char:\n",
    "                    if char in i:\n",
    "                        de_score +=1\n",
    "                if i in stop_germ:\n",
    "                    de_score += 1\n",
    "                if i in stop_en:\n",
    "                    en_score += 1\n",
    "        if de_score>en_score:\n",
    "            return 'german'\n",
    "        elif de_score<en_score:\n",
    "            return 'english'\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_freq(dict):\n",
    "        freq = []\n",
    "        for term, val in dict.items():\n",
    "            freq.append((val, term))\n",
    "        freq = sorted(freq)[::-1]\n",
    "        freq_de = []\n",
    "        freq_en = []\n",
    "        for i,j in freq:\n",
    "            if is_language(j) == 'german':\n",
    "                freq_de.append(j)\n",
    "            else:\n",
    "                freq_en.append(j)\n",
    "        return freq_de, freq_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_de, top_en = top_freq(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    " def language(post):\n",
    "        de_char = ['ä','ö','ü','ß']\n",
    "        de_score = 0\n",
    "        en_score = 0\n",
    "        for i in data_index[post].split():\n",
    "            for char in de_char:\n",
    "                    if char in i:\n",
    "                        de_score +=1\n",
    "            if i in stop_germ or i in top_de[:200]:\n",
    "                de_score += 1\n",
    "            if i in stop_en or i in top_en[:200]:\n",
    "                en_score += 1\n",
    "        if de_score>en_score:\n",
    "            return 'german'\n",
    "        elif en_score>de_score:\n",
    "            return 'english'\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misspells():\n",
    "        words =sorted([key for key in terms.keys()])\n",
    "        de = []\n",
    "        en = []\n",
    "        correct_en = []\n",
    "        correct_de = []\n",
    "        for word in words[:1000]:\n",
    "            g_count = 0\n",
    "            e_count = 0\n",
    "            e_posts = []\n",
    "            g_posts = []\n",
    "            if word in en_dic:\n",
    "                correct_en.append(word)\n",
    "                continue\n",
    "            elif word in de_dic:\n",
    "                correct_de.append(word)\n",
    "                continue\n",
    "            #pos tags for lemmatization generation\n",
    "            tags = ['n','v','a','s','r']\n",
    "            en_lemma = {}\n",
    "            for tag in tags:\n",
    "                lemma = wnl.lemmatize(wnl,word=word, pos=tag) \n",
    "                en_lemma[lemma]= 1\n",
    "#             #German lemmatiziaton/stemming\n",
    "            de_lemma = word\n",
    "            \n",
    "            for post in term_index[word]:\n",
    "                if language(post) == 'german':\n",
    "                    ## get german misspelling count\n",
    "                    if de_lemma not in de_dic:\n",
    "                        if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                            g_count += 1\n",
    "                            g_posts.append(post)\n",
    "                        \n",
    "                elif language(post) == 'english':\n",
    "\n",
    "                    ## get german misspelling count\n",
    "                    if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                        e_count += 1\n",
    "                        e_posts.append(post)\n",
    "                        \n",
    "                else:\n",
    "                    lang = is_language(word)\n",
    "                    if lang == 'german':\n",
    "                        \n",
    "                        ## get german misspelling count    \n",
    "                        if de_lemma not in de_dic:\n",
    "                            if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                                g_count += 1\n",
    "                                g_posts.append(post)\n",
    "                    if lang == 'english':\n",
    "                        ## get english misspelling count\n",
    "                        if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                            e_count += 1\n",
    "                            e_posts.append(post)\n",
    "                            \n",
    "            if g_count > 0:\n",
    "                de.append((g_count, word, g_posts))\n",
    "            if e_count > 0:\n",
    "                en.append((e_count, word, e_posts))\n",
    "        return sorted(de)[::-1], sorted(en)[::-1], correct_de, correct_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_mis, en_mis, terms_de, terms_en = get_misspells()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_damerau(word):\n",
    "    term = ''\n",
    "    for i in range(len(word)):\n",
    "        try:\n",
    "            if word[i] != word[i+2]:\n",
    "                term += word[i]\n",
    "        except: \n",
    "            term += word[i]\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    possible = {}\n",
    "    chunks = [(term[:i], term[i:])for i in range(len(term) + 1)]\n",
    "    for chunk1, chunk2 in chunks:\n",
    "        if chunk2:\n",
    "            possible[chunk1+chunk2[1:]] = 1\n",
    "            for char in alphabet:\n",
    "                possible[chunk1+char+chunk2[1:]] = 1\n",
    "        if len(chunk2) > 1:\n",
    "            possible[chunk1+chunk2[1]+chunk2[0]+chunk2[2:]] = 1\n",
    "        for char in alphabet:\n",
    "            possible[chunk1+char+chunk2] = 1\n",
    "    return possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_suggested(term):\n",
    "    suggestions = en_damerau(term)\n",
    "    suggested = []\n",
    "    for i in suggestions.keys():\n",
    "        tags = ['n','v','a','s','r']\n",
    "        lemmas = []\n",
    "        for tag in tags:\n",
    "            lemmas.append(wnl.lemmatize(wnl,word=i, pos=tag))\n",
    "        lemmas = set(lemmas)\n",
    "        for j in lemmas:\n",
    "            if j in en_dic:\n",
    "                suggested.append(i)\n",
    "    best = [word for word in suggested if lev(term, word) == min(lev(term,word) for word in suggested)]\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mis_en = []\n",
    "for count, word, posts in en_mis[:10]:\n",
    "    top_mis_en.append((word, count, en_suggested(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abput', 2, ['abut', 'about']),\n",
       " ('absolutly', 1, ['absolutely']),\n",
       " ('abolision', 1, []),\n",
       " ('ableism', 1, []),\n",
       " ('abkat', 1, ['abdat', 'abkar']),\n",
       " ('abiten', 1, []),\n",
       " ('abev', 1, ['abe', 'abed', 'abel', 'abet', 'abey']),\n",
       " ('aberta', 1, ['alberta', 'aberia']),\n",
       " ('abdl', 1, ['abel', 'abdal']),\n",
       " ('abdelaziz', 1, [])]"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_mis_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 'aaaah', ['ach'])]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# likely = []\n",
    "# for x in en_mis[:1]:\n",
    "#     term = ''\n",
    "#     for i in range(len(x[1])):\n",
    "#         try:\n",
    "#             if x[1][i] != x[1][i+2]:\n",
    "#                 term += x[1][i]\n",
    "#         except: term += x[1][i]\n",
    "#     tests = [w for w in en_dic if w.startswith(term[0]) and len(w) <= (len(term)+2)]\n",
    "#     matches = [w for w in tests if lev(term, w) == min([lev(term,j) for j in tests])]\n",
    "#     match = [w for w in matches if terms.get(w) and terms[w] == max([terms[w] for w in matches if terms.get(w)])]\n",
    "# #     likely.append((x[0],x[1], match))\n",
    "# likely"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
