{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingsWrapper():\n",
    "    \"\"\"\n",
    "    This postings wrapper creates a link between the index dictionary and the postings list.\n",
    "    \"\"\"\n",
    "    def __init__(self, postings_list, posting, postings_index):\n",
    "        self.frequency = 1\n",
    "        self.postings_index = postings_index\n",
    "        postings_list.append([posting])\n",
    "        \n",
    "\n",
    "    def add_posting(self, postings_list, posting):\n",
    "        \"\"\"\n",
    "        \n",
    "        Adds a posting to the postings list, at correct index according to the term\n",
    "        Only called if the term has yet not corresponding postings.\n",
    "        \n",
    "        :param postings_list: postings list, an attribute of the index.\n",
    "        :param posting: the posting to be added, extracted from a list of tokens and docids.\n",
    "        :return: returns nothing\n",
    "        \"\"\"\n",
    "        if posting not in postings_list[self.postings_index]:\n",
    "            postings_list[self.postings_index].append(posting)\n",
    "            self.frequency += 1\n",
    "\n",
    "de_dic = {}\n",
    "with open('german.dic', 'r', encoding='latin-1') as f:\n",
    "    for row in f:\n",
    "        if len(row) >1:\n",
    "            de_dic[row.strip().lower()] = 1\n",
    "en_dic = {}\n",
    "with open('english.dic', 'r',) as f:\n",
    "    for row in f:\n",
    "        en_dic[row.strip().lower()] = 1\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "\n",
    "class index:\n",
    "    \"\"\"\n",
    "    Processes the tweets.csv file or any file containing the same structure, creates\n",
    "    an inverted index. This is a dictionary terms as keys and an instance of the PostingsWrapper \n",
    "    class as value. Also creates a seperate postings list, also as an attribute, which contains\n",
    "    all tweet ids where each term occured.\n",
    "    \"\"\"\n",
    "    def __init__(self, file):\n",
    "        \"\"\"\n",
    "        :param file: path to tweets.csv file.\n",
    "        \"\"\"\n",
    "        self.data, self.data_index, self.data_dic = self.preprocess(file)\n",
    "        self.index, self.postings_list = self.create_index()\n",
    "        self.top_de, self.top_en = self.top_freq() \n",
    "\n",
    "    def preprocess(self, file):\n",
    "        \"\"\"\n",
    "        Opens raw text, spits it into lines comprised of six columns, stores in intermediary\n",
    "        tab_seperated variable.\n",
    "        Then proceeds to normalize this while transfering it to data variable. Everything is lowered\n",
    "        and compared to a regex which desires to only extract usernames and tokens containing \n",
    "        only letters. All irrelevant columns are disgarded.\n",
    "        \n",
    "        :param file: path to tweets.csv file.\n",
    "        :return: data, containing tweet IDs with corresponding tweets \n",
    "        and a dictionary of all terms and original text\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import string\n",
    "        \n",
    "        raw_text = open(file).read()\n",
    "        tab_seperated = [item.split('\\t') for item in raw_text.split('\\n')]\n",
    "\n",
    "        for line in tab_seperated:\n",
    "            if len(line) == 1:\n",
    "                tab_seperated.remove(line)\n",
    "\n",
    "        data = []\n",
    "        data_index = {}\n",
    "        for i in range(len(tab_seperated)):\n",
    "            data.append([tab_seperated[i][1], tab_seperated[i][4].lower()])\n",
    "            data_index[tab_seperated[i][1]] = tab_seperated[i][4]\n",
    "            \n",
    "        data = data\n",
    "        contractions = [\"it's\", \"he's\",\"she's\",\"that's\", \"what's\", \"there's\",\\\n",
    "                        \"[newline]\", \"'m\", \"'ve\",\"n't\", \"'ll\",\"'re\", \"won't\", \"'d\", \"'s\"]\n",
    "        fixes = [\"it is\", \"he is\",\"she is\",\"that is\", \"what is\", \"there is\",\\\n",
    "                 \" \", \" am\", \" have\", \" not\", \" will\", \" are\", \"will not\", \" would\", \"\"]\n",
    "        for line in data:\n",
    "            for i in range(len(contractions)):\n",
    "                if contractions[i] in line[1]:\n",
    "                    line[1]= line[1].replace(contractions[i], fixes[i])\n",
    "            line[1] = re.sub(r'[^\\w\\s]', ' ' , line[1])\n",
    "            line[1] = re.sub(r'[0-9].*\\s', ' ' , line[1])\n",
    "            line[1] = re.sub(r'https?.+\\s', ' ' , line[1])\n",
    "            line[1] = re.sub(r'[\\W].+[^\\W\\s]+|[^ ]+\\.[^ ]+ |[^a-zA-Zäöüß\\s]+ \\\n",
    "                             | \\d+|[^\\w\\s]+.[^\\W\\s]+| https?','', line[1])\n",
    "            #line[1] = re.sub('https?:\\/\\/[^\\s]*|[^a-z\\s]', '', line[1])\n",
    "        data_dic = {}\n",
    "        for row in data:\n",
    "            data_dic[row[0]] = row[1]\n",
    "\n",
    "        return data, data_index, data_dic\n",
    "\n",
    "    def create_index(self):\n",
    "        \"\"\"\n",
    "        Creates the index and postings list.\n",
    "        :return: index, a dictionary having a unique term as key and a PostingsWrapper instance\n",
    "        as value, and postings_list, a large list of lists containing all postings for each unique\n",
    "        term.\n",
    "        \"\"\"\n",
    "\n",
    "        # We initialize the index, the postings list, and an intermediary tokens_and_ids variable.\n",
    "        index = {}\n",
    "        postings_list = []\n",
    "        tokens_and_ids = []\n",
    "\n",
    "        # For each line in data, we split each tweet by whitespace into tokens.\n",
    "        # As a simple preprocessing step we check to make sure that the length of each token is\n",
    "        # > 0 before appending the token and its tweet ID to the tokens_and_ids list.\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_en = set(stopwords.words('english'))\n",
    "        stop_germ = set(stopwords.words('german'))\n",
    "        \n",
    "        for line in self.data:\n",
    "            for token in [x for x in line[1].split()  if (not x in stop_en) and (not x in stop_germ)]:\n",
    "                if len(token) > 1:\n",
    "                    tokens_and_ids.append([token, line[0]])\n",
    "\n",
    "        # We sort our list of all tokens.\n",
    "        \n",
    "        tokens_and_ids.sort()\n",
    "\n",
    "        # The postings_index variable we initialize here will be used as we instantiate\n",
    "        # PostingsWrapper objects. This integer will enable us to keep track of the index\n",
    "        # of the postings list where all of a given term's postings are contained.\n",
    "        \n",
    "        postings_index = 0\n",
    "        \n",
    "        # For each line in tokens_and_ids, we check to make sure it is not already in our index.\n",
    "        # If it is not we add it, create a corresponding PostingsWrapper Object that will\n",
    "        # add to the postings list as it is initialized. The PostingsWrapper will also keep track\n",
    "        # of frequency for us.\n",
    "        # Having done this we then increment the postings_index variable by 1.\n",
    "        # If it is found that the term is already present in our index, we simply add the new \n",
    "        # posting to its postings list using the PostingsWrapper.add_posting method.\n",
    "        for line in tokens_and_ids:\n",
    "            if line[0] not in index.keys():\n",
    "                index[line[0]] = PostingsWrapper(postings_list, line[1], postings_index)\n",
    "                postings_index += 1\n",
    "            else:\n",
    "                 index[line[0]].add_posting(postings_list, line[1])\n",
    "\n",
    "        return index, postings_list\n",
    "            \n",
    "    def get_frequency(self, term):\n",
    "        \"\"\"\n",
    "        Pulls frequency from wrapper of term\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return index.index[term].frequency\n",
    "        except:\n",
    "            print('Term not found.')\n",
    "    \n",
    "    def All_frequencies(self):\n",
    "        '''\n",
    "        return the term and frequencies in descending order\n",
    "        '''\n",
    "        frequencies = []\n",
    "        for term in self.index.keys():\n",
    "            frequencies.append((self.index[term].frequency, term))\n",
    "        return sorted(frequencies)[::-1]\n",
    "    def top_freq(self):\n",
    "        freq_de = []\n",
    "        freq_en = []\n",
    "        frequencies = self.All_frequencies()[:200]\n",
    "        for i,j in frequencies:\n",
    "            if self.is_language(j) == 'german':\n",
    "                freq_de.append(j)\n",
    "            else:\n",
    "                freq_en.append(j)\n",
    "        return freq_de, freq_en\n",
    "    \n",
    "    def is_language(self,term):\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "        stop_en = set(stopwords.words('english'))\n",
    "        stop_germ = set(stopwords.words('german'))\n",
    "        \n",
    "        g_score = 0\n",
    "        en_score = 0\n",
    "        for post in self.postings_list[self.index[term].postings_index]:\n",
    "            for i in self.data_index[post].lower().strip().split():\n",
    "                if i in stop_germ:\n",
    "                    g_score += 1\n",
    "                if i in stop_en:\n",
    "                    en_score += 1\n",
    "#         print('DE:', g_score,'\\t','EN:',en_score)\n",
    "        if g_score>en_score:\n",
    "            return 'german'\n",
    "#             print('probability:', (g_score/(g_score+en_score)))\n",
    "#             print(\"german\")\n",
    "        elif g_score<en_score:\n",
    "            return 'english'\n",
    "#             print('probability:', (en_score/(g_score+en_score)))\n",
    "#             print('english')\n",
    "        else:\n",
    "            return 'unsure'\n",
    "    \n",
    "    def language(self,post):\n",
    "        from nltk.corpus import stopwords\n",
    "        import re\n",
    "        contractions = [\"it's\", \"he's\",\"she's\",\"that's\", \"what's\", \"there's\",\\\n",
    "                        \"[newline]\", \"'m\", \"'ve\",\"n't\", \"'ll\",\"'re\", \"won't\", \"'d\", \"'s\"]\n",
    "        fixes = [\"it is\", \"he is\",\"she is\",\"that is\", \"what is\", \"there is\",\\\n",
    "                 \" \", \" am\", \" have\", \" not\", \" will\", \" are\", \"will not\", \" would\", \"\"]\n",
    "        stop_en = set(stopwords.words('english'))\n",
    "        stop_germ = set(stopwords.words('german'))\n",
    "        de_char = ['ä','ö','ü','ß']\n",
    "        g_score = 0\n",
    "        en_score = 0\n",
    "        \n",
    "        for char in self.data_index[post].lower():\n",
    "            if char in de_char:\n",
    "                g_score +=1\n",
    "\n",
    "        text = self.data_index[post].lower()\n",
    "        for x in range(len(contractions)):\n",
    "            text = text.replace(contractions[x], fixes[x])\n",
    "        text = re.sub(r'[^\\w\\s]','', text)\n",
    "        for i in text.strip().split():    \n",
    "            if i in stop_germ or i in self.top_de:\n",
    "                g_score += 1\n",
    "            if i in stop_en or i in self.top_en:\n",
    "                en_score += 1\n",
    "        #print(post, 'DE:', g_score,'\\t','EN:',en_score)\n",
    "        if g_score>en_score:\n",
    "            return 'german'\n",
    "#             print('probability:', (g_score/(g_score+en_score)))\n",
    "#             print(\"german\")\n",
    "        elif en_score>g_score:\n",
    "            return 'english'\n",
    "#             print('probability:', (en_score/(g_score+en_score)))\n",
    "#             print('english')\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_misspells(self):\n",
    "        #from nltk.stem import GermanWortschatzLemmatizer as gwl\n",
    "        terms = sorted(self.index.keys())\n",
    "        de = []\n",
    "        en = []\n",
    "        for x in terms[:200]:\n",
    "            term = ''\n",
    "            for i in range(len(x)):\n",
    "                try:\n",
    "                    if x[i] != [i+2]:\n",
    "                        term += x[i]\n",
    "                except:\n",
    "                    term += x[i]\n",
    "            g_count = 0\n",
    "            e_count = 0\n",
    "            e_posts = []\n",
    "            g_posts = []\n",
    "            tags = ['n','v','a','s','r']\n",
    "            en_lemma = []\n",
    "            for tag in tags:\n",
    "                en_lemma.append(wnl.lemmatize(wnl,word=term, pos=tag))\n",
    "            en_lemma = set(en_lemma)\n",
    "            de_lemma = term\n",
    "            for post in self.postings_list[self.index[term].postings_index]:\n",
    "                if self.language(post) == 'german':\n",
    "                    ## Apply Lematizer to word here\n",
    "                    ## get german misspelling count\n",
    "                    if de_lemma not in de_dic:\n",
    "#                         for i in en_lemma:\n",
    "#                             if i in en_dic:\n",
    "#                                 continue    \n",
    "                        g_count += 1\n",
    "                        g_posts.append(post)\n",
    "                elif self.language(post) == 'english':\n",
    "                    ## Apply Lematizer to word here\n",
    "                    ## get german misspelling count\n",
    "                    if len([i for i in en_lemma if i in en_dic]) == 0:\n",
    "#                     if de_lemma not in de_dic:\n",
    "                        e_count += 1\n",
    "                        e_posts.append(post)\n",
    "                else:\n",
    "                    lang = self.is_language(term)\n",
    "                    if lang == None:\n",
    "                        continue\n",
    "                    if lang == 'german':\n",
    "                        ## Apply Lematizer to word here\n",
    "                        ## get german misspelling count    \n",
    "                        if de_lemma not in de_dic:\n",
    "#                             for i in en_lemma:\n",
    "#                                 if i in en_dic:\n",
    "#                                     continue\n",
    "                            g_count += 1\n",
    "                            g_posts.append(post)\n",
    "                    else:\n",
    "                        ## Apply Lematizer to word here\n",
    "                        ## get german misspelling count\n",
    "                        if len([i for i in en_lemma if i in en_dic]) == 0:\n",
    "#                             if de_lemma not in de_dic:\n",
    "                            e_count += 1\n",
    "                            e_posts.append(post)\n",
    "                            \n",
    "            if g_count > 0:\n",
    "                de.append((g_count, term, g_posts))\n",
    "            if e_count > 0:\n",
    "                en.append((e_count, term, e_posts))\n",
    "        return sorted(de)[::-1], sorted(en)[::-1]\n",
    "    \n",
    "    def en_damerau(self, term):\n",
    "        alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        term = term.lower()\n",
    "        possible = {}\n",
    "        chunks = [(term[:i], term[i:])for i in range(len(term) + 1)]\n",
    "        for chunk1, chunk2 in chunks:\n",
    "            if chunk2:\n",
    "                possible[chunk1+chunk2[1:]] = 1\n",
    "                for char in alphabet:\n",
    "                    possible[chunk1+char+chunk2[1:]] = 1\n",
    "            if len(chunk2) > 1:\n",
    "                possible[chunk1+chunk2[1]+chunk2[0]+chunk2[2:]] = 1\n",
    "            for char in alphabet:\n",
    "                possible[chunk1+char+chunk2] = 1\n",
    "        return possible\n",
    "    \n",
    "    def de_damerau(self, term):\n",
    "        alphabet = \"abcdefghijklmnopqrstuvwxyzäöüß\"\n",
    "        term = term.lower()\n",
    "        splits     = [(term[:i], term[i:])    for i in range(len(term) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in alphabet]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in alphabet]\n",
    "        return set(deletes + transposes + replaces + inserts)    \n",
    "\n",
    "    def en_suggested(self,term):\n",
    "        suggestions = self.en_damerau(term)\n",
    "        suggested = []\n",
    "        for i in suggestions.keys():\n",
    "            tags = ['n','v','a','s','r']\n",
    "            lemmas = []\n",
    "            for tag in tags:\n",
    "                lemmas.append(wnl.lemmatize(wnl,word=i, pos=tag))\n",
    "            lemmas = set(lemmas)\n",
    "            for j in lemmas:\n",
    "                if j in en_dic:\n",
    "                    suggested.append(i)\n",
    "        best = []\n",
    "        count = 0\n",
    "        for k in set(suggested):\n",
    "            counter = 0\n",
    "            for h in k:\n",
    "                if h in k:\n",
    "                    counter +=1\n",
    "            if counter >= count and counter < len(i):\n",
    "                count = counter\n",
    "                best.append(k)\n",
    "        return best\n",
    "#             if i in en_dic:\n",
    "#                 suggested.append(i)\n",
    "#         return suggested\n",
    "    \n",
    "    def de_suggested(self,term):\n",
    "        suggestions = self.de_damerau(term)\n",
    "        suggested = []\n",
    "        for i in suggestions:\n",
    "            if i in de_dic:\n",
    "                suggested.append(i)\n",
    "        return suggested\n",
    "        \n",
    "    def spell_check(self, lst):\n",
    "        de_dic = []\n",
    "        with open('german.dic', 'r', encoding='latin-1') as f:\n",
    "            for row in f:\n",
    "                if len(row) >1:\n",
    "                    de_dic.append(row.strip().lower())\n",
    "        en_dic = []\n",
    "        with open('english.dic', 'r',) as f:\n",
    "            for row in f:\n",
    "                en_dic.append(row.strip().lower())\n",
    "        for post in lst:\n",
    "            if self.language(post) == 'german':\n",
    "                print('German')\n",
    "                misspells = [i for i in self.data_dic[post].split() if i not in de_dic] \n",
    "            else:\n",
    "                print('English')\n",
    "                misspells = [i for i in self.data_dic[post].split() if i not in en_dic]\n",
    "            print(\"Number of misspellings:\", len(misspells))\n",
    "            print(misspells)\n",
    "        return\n",
    "    \n",
    "    def query_one(self, term):\n",
    "        \"\"\"\n",
    "        Queries for a term.\n",
    "        :param term: query term\n",
    "        :return: postings list corresponding to query term, or error message if no results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.postings_list[index.index[term].postings_index]\n",
    "        except:\n",
    "            print('No results for query.')\n",
    "#         try:\n",
    "#             for posting in self.postings_list[index.index[term].postings_index]:\n",
    "#                 print(posting, self.data_index[posting], '\\n')\n",
    "#         except:\n",
    "#             print('No results for query.')\n",
    "\n",
    "\n",
    "    def query_and(self, term1, term2):\n",
    "        \"\"\"\n",
    "        Queries for the intersection of two terms.\n",
    "        :param term1: first term\n",
    "        :param term2: second term\n",
    "        :return: returns intersection of postings lists of both terms.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Here we compare the two lists and create iterators to help us compare the two postings lists\n",
    "        def And(post1,post2):\n",
    "            if len(post1) < len(post2):    \n",
    "                iterpost1 = iter(post1)\n",
    "                iterpost2 = iter(post2)\n",
    "            else:\n",
    "                iterpost1 = iter(post2)\n",
    "                iterpost2 = iter(post1)\n",
    "                \n",
    "        # Here we initialize an empty intersection variable which will (hopefully) be filled.\n",
    "            intersection = []\n",
    "            \n",
    "            current1 = next(iterpost1)\n",
    "            current2 = next(iterpost2)\n",
    "        # This is the loop that iterates over the members of each postings list, comparing them.\n",
    "        # If there is a match it will be added to the intersection.\n",
    "            while True:\n",
    "                if current1 == current2:\n",
    "                    intersection.append(current1)\n",
    "                    try:\n",
    "                        current1 = next(iterpost1)\n",
    "                        current2 = next(iterpost2)\n",
    "                    except:\n",
    "                        break\n",
    "                elif current1 < current2:\n",
    "                    try:\n",
    "                        current1 = next(iterpost1)\n",
    "                    except:\n",
    "                        break\n",
    "                else:\n",
    "                    try:\n",
    "                        current2 = next(iterpost2)\n",
    "                    except:\n",
    "                        break\n",
    "            # Here we print each text and id number found in intersection\n",
    "#             if len(intersection) != 0:\n",
    "#                 for i in intersection:\n",
    "#                     print( i, self.data_index[i], '\\n')\n",
    "                    \n",
    "#             else:\n",
    "#                 print('No results for query.')\n",
    "            # Here we access the postings list for each term, assign them to variables.\n",
    "#         if type(term1) == list:\n",
    "#             try:\n",
    "#                 postings2 = self.postings_list[index.index[term2].postings_index]\n",
    "#                 return And(term1, postings2)\n",
    "#             except:\n",
    "#                 return None\n",
    "        try:\n",
    "            postings1 = self.postings_list[index.index[term1].postings_index]\n",
    "            postings2 = self.postings_list[index.index[term2].postings_index]\n",
    "            return And(postings1, postings2)\n",
    "        except:\n",
    "            print('Error: 1 or more terms not found.')\n",
    "#     def query_three(self, term1, term2, term3):\n",
    "#         self.query_and(self.query_and(term1,term2),term3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4fd74b067819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrenou_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-bd49cc871589>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \"\"\"\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostings_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-bd49cc871589>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mtab_seperated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tweets.csv'"
     ]
    }
   ],
   "source": [
    "renou_index = index('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('renou_index5.pkl', 'wb') as f:\n",
    "    pickle.dump(renou_index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('renou_index5.pkl', 'rb') as f:\n",
    "    index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frog\n"
     ]
    }
   ],
   "source": [
    "a = 'frog'\n",
    "new_a = ''\n",
    "for i in range(len(a)):\n",
    "    try:\n",
    "        if a[i] != a[i+2]:\n",
    "            new_a += a[i]\n",
    "    except:\n",
    "        new_a += a[i]\n",
    "print(new_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_miss, en_miss = index.get_misspells()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#de_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'german'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.language('1001528572438249472')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Acun \"Alp atamıyor\" diyemez ki...Biliyor çünkü Alp onun sesini ayırt edeceğini. Kıyamaz ki #survivor2018 #hilmur https://t.co/FOLgUgSGR5'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.data_index['980858193700995072']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'german'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.language('1002324883651678208')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Actually kann ich jeden verstehen der mit League aufgehört habe..[NEWLINE]Wenn in der CS gesagt wird, dass deine Mutter hoffentlich einen Tumor hat dann weiß ich wirklich nicht was los ist.[NEWLINE]@RiotSupport @riotgames Wieso ist das Reportsystem einfach aus 2001?'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.data_index['1002324883651678208']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dah',\n",
       " 'wah',\n",
       " 'aal',\n",
       " 'auh',\n",
       " 'bah',\n",
       " 'mah',\n",
       " 'aas',\n",
       " 'aam',\n",
       " 'ash',\n",
       " 'pah',\n",
       " 'sah',\n",
       " 'ach',\n",
       " 'rah',\n",
       " 'aha',\n",
       " 'hah',\n",
       " 'yah']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.en_suggested('aah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'activin' in en_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer as wnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerates\n",
      "accelerate\n",
      "accelerates\n",
      "accelerates\n",
      "accelerates\n"
     ]
    }
   ],
   "source": [
    "tags = ['n','v','r','a','s']\n",
    "for tag in tags:\n",
    "    print(wnl.lemmatize(wnl,word='accelerates', pos=tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = index.All_frequencies()[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_de = []\n",
    "freq_en = []\n",
    "for i,j in test:\n",
    "    if index.is_language(j) == 'german':\n",
    "        freq_de.append(j)\n",
    "    else:\n",
    "        freq_en.append(j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10003"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lien',\n",
       " 'lier',\n",
       " 'like',\n",
       " 'link',\n",
       " 'lief',\n",
       " 'leek',\n",
       " 'lick',\n",
       " 'lied',\n",
       " 'miek',\n",
       " 'lieu',\n",
       " 'lies',\n",
       " 'lisk']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.en_suggested('liek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dic = {}\n",
    "with open('english.dic', 'r', encoding='latin-1') as f:\n",
    "    for row in f:\n",
    "        if len(row) >1:\n",
    "            en_dic[row.strip().lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.api import StemmerI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import GermanStemmer as gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1954112"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234371"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = {}\n",
    "for i in de_dic.keys():\n",
    "    i = gs(i)\n",
    "    de[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
