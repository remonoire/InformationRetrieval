{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "from nltk.stem.snowball import GermanStemmer as gs\n",
    "from nltk.metrics.distance import edit_distance as lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = [\"´\",\"‘\",\"’\",\"it's\", \"he's\",\"she's\",\"that's\", \"what's\", \"there's\", \"aren't\",\\\n",
    "            \"[newline]\", \"'m\", \"'ve\",\"n't\", \"'ll\",\"'re\", \"won't\", \"'d\",\"geht's\",\"gibt's\", \"'s\", ' xd']\n",
    "fixes = [\"'\",\"'\",\"'\",\"it is\", \"he is\",\"she is\",\"that is\", \"what is\", \"there is\",\"are not\",\\\n",
    "                 \" \", \" am\", \" have\", \" not\", \" will\", \" are\", \"will not\", \" would\",\"geht es\",\"gibt es\", \"\", ' ']\n",
    "stop_en = set(stopwords.words('english'))\n",
    "stop_germ = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dic = {}\n",
    "with open('german.dic', 'r', encoding='latin-1') as f:\n",
    "    for row in f:\n",
    "        if len(row) >1:\n",
    "            de_dic[row.strip().lower()] = 1\n",
    "en_dic = {}\n",
    "with open('english.dic', 'r',) as f:\n",
    "    for row in f:\n",
    "        en_dic[row.strip().lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words that are not in dictionary but should be\n",
    "en_dic['anymore'] = 1\n",
    "en_dic['adhd'] = 1\n",
    "en_dic['asshole'] = 1\n",
    "en_dic['fucking'] = 1\n",
    "en_dic['porn'] = 1\n",
    "en_dic['lol'] = 1\n",
    "en_dic['omg'] = 1\n",
    "en_dic['fuck'] = 1\n",
    "en_dic['proud'] = 1\n",
    "en_dic['others'] = 1\n",
    "en_dic['mom'] = 1\n",
    "en_dic['ptsd'] = 1\n",
    "en_dic['europe']= 1\n",
    "en_dic['tumour'] = 1\n",
    "en_dic['tumours'] = 1\n",
    "en_dic['stats'] = 1\n",
    "en_dic['favourite'] = 1\n",
    "en_dic['boyfriend'] = 1\n",
    "en_dic['fortnite'] = 1 #game name\n",
    "\n",
    "#acronys\n",
    "en_dic['af'] = 1 #as fuck\n",
    "en_dic['tbh'] = 1 #to be honest\n",
    "en_dic['bc'] = 1 #because\n",
    "en_dic['idk'] = 1 #i don't know\n",
    "en_dic['rn'] = 1 #right now\n",
    "en_dic['ppl'] = 1 #people\n",
    "en_dic['lmao'] = 1 #laugh my ass off\n",
    "en_dic['wtf'] = 1 #what the fuck\n",
    "en_dic['btw'] = 1 #by the way\n",
    "en_dic['pls'] = 1 #please\n",
    "en_dic['thx'] = 1 #thanks\n",
    "#en_dic['aml'] = 1 #anti-money laundering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = open('tweets.csv').read()\n",
    "tab_seperated = [item.split('\\t') for item in raw_text.split('\\n') if len(item.split('\\t')) >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for i in range(len(tab_seperated)):\n",
    "    data[tab_seperated[i][1]] = tab_seperated[i][4]\n",
    "    \n",
    "data_index = data.copy()    \n",
    "for tweet in data_index.keys():\n",
    "    data_index[tweet] = data_index[tweet].lower()\n",
    "    for i in range(len(contractions)):\n",
    "        if contractions[i] in data_index[tweet]:\n",
    "            data_index[tweet] = data_index[tweet].replace(contractions[i], fixes[i])\n",
    "    data_index[tweet] = re.sub('https?[^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub('[@#][^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[0-9][^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'\\w+\\.[^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[^a-zäöüß\\s]', ' ', data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[^\\w\\s]', ' ' , data_index[tweet])\n",
    "    \n",
    "terms = {}\n",
    "term_index = {}\n",
    "for num,tweet in data_index.items():\n",
    "    for word in tweet.split():\n",
    "        if word in term_index:\n",
    "            term_index[word].append(num)\n",
    "        elif word not in term_index:\n",
    "            term_index[word] = [num]\n",
    "        if word in terms:\n",
    "            terms[word] += 1\n",
    "        else:\n",
    "            terms[word] = 1\n",
    "\n",
    "for key in term_index.keys():\n",
    "    term_index[key] = sorted(term_index[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_language(term):\n",
    "        de_char = ['ä','ö','ü','ß']\n",
    "        de_score = 0\n",
    "        en_score = 0\n",
    "        for post in term_index[term]:\n",
    "            for i in data_index[post].strip().split():\n",
    "                for char in de_char:\n",
    "                    if char in i:\n",
    "                        de_score +=1\n",
    "                if i in stop_germ:\n",
    "                    de_score += 1\n",
    "                if i in stop_en:\n",
    "                    en_score += 1\n",
    "        if de_score>en_score:\n",
    "            return 'german'\n",
    "        elif de_score<en_score:\n",
    "            return 'english'\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_freq(dict):\n",
    "        freq = []\n",
    "        for term, val in dict.items():\n",
    "            freq.append((val, term))\n",
    "        freq = sorted(freq)[::-1]\n",
    "        freq_de = []\n",
    "        freq_en = []\n",
    "        for i,j in freq:\n",
    "            if is_language(j) == 'german':\n",
    "                freq_de.append(j)\n",
    "            else:\n",
    "                freq_en.append(j)\n",
    "        return freq_de, freq_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_de, top_en = top_freq(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misspells():\n",
    "        words =sorted([key for key in terms.keys()])\n",
    "        de = []\n",
    "        en = []\n",
    "        correct_en = []\n",
    "        correct_de = []\n",
    "        for word in words:\n",
    "            g_count = 0\n",
    "            e_count = 0\n",
    "            e_posts = []\n",
    "            g_posts = []\n",
    "            if word in en_dic:\n",
    "                correct_en.append(word)\n",
    "                continue\n",
    "            elif word in de_dic:\n",
    "                correct_de.append(word)\n",
    "                continue\n",
    "            #pos tags for lemmatization generation\n",
    "            tags = ['n','v','a','s','r']\n",
    "            en_lemma = {}\n",
    "            for tag in tags:\n",
    "                lemma = wnl.lemmatize(wnl,word=word, pos=tag) \n",
    "                en_lemma[lemma]= 1\n",
    "#             #German lemmatiziaton/stemming\n",
    "            de_lemma = word\n",
    "            \n",
    "            for post in term_index[word]:\n",
    "                if language(post) == 'german':\n",
    "                    ## get german misspelling count\n",
    "                    if de_lemma not in de_dic:\n",
    "                        if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                            g_count += 1\n",
    "                            g_posts.append(post)\n",
    "                        \n",
    "                elif language(post) == 'english':\n",
    "\n",
    "                    ## get german misspelling count\n",
    "                    if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                        e_count += 1\n",
    "                        e_posts.append(post)\n",
    "                        \n",
    "                else:\n",
    "                    lang = is_language(word)\n",
    "                    if lang == 'german':\n",
    "                        \n",
    "                        ## get german misspelling count    \n",
    "                        if de_lemma not in de_dic:\n",
    "                            if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                                g_count += 1\n",
    "                                g_posts.append(post)\n",
    "                    if lang == 'english':\n",
    "                        ## get english misspelling count\n",
    "                        if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                            e_count += 1\n",
    "                            e_posts.append(post)\n",
    "                            \n",
    "            if g_count > 0:\n",
    "                de.append((g_count, word, g_posts))\n",
    "            if e_count > 0:\n",
    "                en.append((e_count, word, e_posts))\n",
    "        return sorted(de)[::-1], sorted(en)[::-1], correct_de, correct_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('de_stemmed.plk', 'wb') as f:\n",
    "#     pickle.dump(de_stemmed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    " def language(post):\n",
    "        de_char = ['ä','ö','ü','ß']\n",
    "        de_score = 0\n",
    "        en_score = 0\n",
    "        for i in data_index[post].split():\n",
    "            for char in de_char:\n",
    "                    if char in i:\n",
    "                        de_score +=1\n",
    "            if i in stop_germ or i in top_de[:200]:\n",
    "                de_score += 1\n",
    "            if i in stop_en or i in top_en[:200]:\n",
    "                en_score += 1\n",
    "        if de_score>en_score:\n",
    "            return 'german'\n",
    "        elif en_score>de_score:\n",
    "            return 'english'\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_mis, en_mis, terms_de, terms_en = get_misspells()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_damerau(word):\n",
    "    term = ''\n",
    "    for i in range(len(word)):\n",
    "        try:\n",
    "            if word[i] != word[i+2]:\n",
    "                term += word[i]\n",
    "        except: \n",
    "            term += word[i]\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    possible = {}\n",
    "    chunks = [(term[:i], term[i:])for i in range(len(term) + 1)]\n",
    "    for chunk1, chunk2 in chunks:\n",
    "        if chunk2:\n",
    "            possible[chunk1+chunk2[1:]] = 1\n",
    "            for char in alphabet:\n",
    "                possible[chunk1+char+chunk2[1:]] = 1\n",
    "        if len(chunk2) > 1:\n",
    "            possible[chunk1+chunk2[1]+chunk2[0]+chunk2[2:]] = 1\n",
    "        for char in alphabet:\n",
    "            possible[chunk1+char+chunk2] = 1\n",
    "    return possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_suggested(term):\n",
    "    if term == 'kinda':\n",
    "        return \"kind of\"\n",
    "    if term == 'gonna':\n",
    "        return 'going to'\n",
    "    if term == 'wanna':\n",
    "        return 'want to'\n",
    "    suggestions = en_damerau(term)\n",
    "    suggested = []\n",
    "    for i in suggestions.keys():\n",
    "        tags = ['n','v','a','s','r']\n",
    "        lemmas = []\n",
    "        for tag in tags:\n",
    "            lemmas.append(wnl.lemmatize(wnl,word=i, pos=tag))\n",
    "        lemmas = set(lemmas)\n",
    "        for j in lemmas:\n",
    "            if j in en_dic:\n",
    "                suggested.append(i)\n",
    "    refined = [word for word in suggested if lev(term, word) == min(lev(term,word) for word in suggested)]\n",
    "#     if len(refined) == 0:\n",
    "#         \n",
    "    try:\n",
    "        best = sorted([(terms[word],word) for word in refined if word in top_en])[::-1]\n",
    "        return best[0][1]\n",
    "    except:\n",
    "        return refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_damerau(word):\n",
    "    term = ''\n",
    "    for i in range(len(word)):\n",
    "        try:\n",
    "            if word[i] != word[i+2]:\n",
    "                term += word[i]\n",
    "        except: \n",
    "            term += word[i]\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyzäöüß\"\n",
    "    possible = {}\n",
    "    chunks = [(term[:i], term[i:])for i in range(len(term) + 1)]\n",
    "    for chunk1, chunk2 in chunks:\n",
    "        if chunk2:\n",
    "            possible[chunk1+chunk2[1:]] = 1\n",
    "            for char in alphabet:\n",
    "                possible[chunk1+char+chunk2[1:]] = 1\n",
    "        if len(chunk2) > 1:\n",
    "            possible[chunk1+chunk2[1]+chunk2[0]+chunk2[2:]] = 1\n",
    "        for char in alphabet:\n",
    "            possible[chunk1+char+chunk2] = 1\n",
    "    return possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_suggested(term):\n",
    "    suggestions = de_damerau(term)\n",
    "    suggested = []\n",
    "    for i in suggestions.keys():\n",
    "        tags = ['n','v','a','s','r']\n",
    "        lemmas = []\n",
    "        for tag in tags:\n",
    "            lemmas.append(wnl.lemmatize(wnl,word=i, pos=tag))\n",
    "        lemmas = set(lemmas)\n",
    "        for j in lemmas:\n",
    "            if j in en_dic:\n",
    "                suggested.append(i)\n",
    "    refined = [word for word in suggested if lev(term, word) == min(lev(term,word) for word in suggested)]\n",
    "#     if len(refined) == 0:\n",
    "#         \n",
    "    try:\n",
    "        best = sorted([(terms[word],word) for word in refined if word in top_de])[::-1]\n",
    "        return best[0][1]\n",
    "    except:\n",
    "        return refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mis_en = []\n",
    "for count, word, posts in en_mis[:25]:\n",
    "    top_mis_en.append((word, count, en_suggested(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kinda', 2295, 'kind of'),\n",
       " ('gonna', 494, 'going to'),\n",
       " ('wanna', 330, 'want to'),\n",
       " ('bts', 246, 'its'),\n",
       " ('ng', 220, 'no'),\n",
       " ('que', 180, 'due'),\n",
       " ('nh', 124, 'no'),\n",
       " ('tr', 97, 'to'),\n",
       " ('ve', 94, 'me'),\n",
       " ('cuz', 93, 'cut'),\n",
       " ('lc', 88, 'c'),\n",
       " ('gotta', 85, 'lotta'),\n",
       " ('bro', 85, 'bra'),\n",
       " ('le', 84, 'me'),\n",
       " ('pd', 83, 'ed'),\n",
       " ('rt', 74, 'it'),\n",
       " ('il', 74, 'i'),\n",
       " ('congrats', 71, []),\n",
       " ('hpv', 66, []),\n",
       " ('jimin', 65, 'join'),\n",
       " ('fav', 64, 'fan'),\n",
       " ('yoongi', 61, []),\n",
       " ('crc', 60, 'arc'),\n",
       " ('por', 59, 'for'),\n",
       " ('gbm', 59, 'gym')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_mis_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mis_de = []\n",
    "for count, word, posts in de_mis[:25]:\n",
    "    top_mis_de.append((word, count, de_suggested(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sowas', 500, 'sowss'),\n",
       " ('nen', 425, 'den'),\n",
       " ('nochmal', 294, []),\n",
       " ('nem', 234, 'ne'),\n",
       " ('erstmal', 216, []),\n",
       " ('daß', 183, 'das'),\n",
       " ('gibts', 164, 'gibs'),\n",
       " ('nich', 151, 'ich'),\n",
       " ('zb', 147, 'ab'),\n",
       " ('vllt', 147, 'volt'),\n",
       " ('gehts', 135, ['gets', 'gelts', 'gents']),\n",
       " ('bir', 133, 'mir'),\n",
       " ('bmd', 129, ['bad', 'bed', 'bid', 'bod', 'bud']),\n",
       " ('ner', 119, 'der'),\n",
       " ('det', 105, 'der'),\n",
       " ('hahaha', 98, 'aha'),\n",
       " ('würd', 96, 'wird'),\n",
       " ('erdogan', 94, []),\n",
       " ('hcc', 78, 'hic'),\n",
       " ('dat', 72, 'das'),\n",
       " ('heut', 70, 'hut'),\n",
       " ('creepnekk', 66, []),\n",
       " ('sooo', 64, 'soso'),\n",
       " ('immernoch', 64, []),\n",
       " ('imdb', 63, [])]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_mis_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
