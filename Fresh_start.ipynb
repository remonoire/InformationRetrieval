{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "from nltk.stem.snowball import GermanStemmer as gs\n",
    "from nltk.metrics.distance import edit_distance as lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = [\"´\",\"‘\",\"’\",\"it's\", \"he's\",\"she's\",\"that's\", \"what's\", \"there's\", \"aren't\",\\\n",
    "            \"[newline]\", \"'m\", \"'ve\",\"n't\", \"'ll\",\"'re\", \"won't\", \"'d\",\"geht's\",\"gibt's\", \"'s\", ' xd']\n",
    "fixes = [\"'\",\"'\",\"'\",\"it is\", \"he is\",\"she is\",\"that is\", \"what is\", \"there is\",\"are not\",\\\n",
    "                 \" \", \" am\", \" have\", \" not\", \" will\", \" are\", \"will not\", \" would\",\"geht es\",\"gibt es\", \"\", ' ']\n",
    "stop_en = set(stopwords.words('english'))\n",
    "stop_germ = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_dic = {}\n",
    "with open('german.dic', 'r', encoding='latin-1') as f:\n",
    "    for row in f:\n",
    "        if len(row) >1:\n",
    "            de_dic[row.strip().lower()] = 1\n",
    "en_dic = {}\n",
    "with open('english.dic', 'r',) as f:\n",
    "    for row in f:\n",
    "        en_dic[row.strip().lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words that are not in en dictionary but should be\n",
    "en_dic['anymore'] = 1\n",
    "en_dic['adhd'] = 1\n",
    "en_dic['asshole'] = 1\n",
    "en_dic['fucking'] = 1\n",
    "en_dic['porn'] = 1\n",
    "en_dic['fuck'] = 1\n",
    "en_dic['proud'] = 1\n",
    "en_dic['others'] = 1\n",
    "en_dic['mom'] = 1\n",
    "en_dic['ptsd'] = 1\n",
    "en_dic['europe']= 1\n",
    "en_dic['tumour'] = 1\n",
    "en_dic['tumours'] = 1\n",
    "en_dic['stats'] = 1\n",
    "en_dic['favourite'] = 1\n",
    "en_dic['boyfriend'] = 1\n",
    "en_dic['fortnite'] = 1 #game name\n",
    "en_dic['bts'] = 1 # Korean pop group\n",
    "\n",
    "#acronys\n",
    "# en_dic['lol'] = 1 #laugh out loud\n",
    "# en_dic['omg'] = 1 #oh my god\n",
    "# en_dic['af'] = 1 #as fuck\n",
    "# en_dic['tbh'] = 1 #to be honest\n",
    "# en_dic['bc'] = 1 #because\n",
    "# en_dic['idk'] = 1 #i don't know\n",
    "# en_dic['rn'] = 1 #right now\n",
    "# en_dic['ppl'] = 1 #people\n",
    "# en_dic['lmao'] = 1 #laugh my ass off\n",
    "# en_dic['wtf'] = 1 #what the fuck\n",
    "# en_dic['btw'] = 1 #by the way\n",
    "# en_dic['pls'] = 1 #please\n",
    "# en_dic['thx'] = 1 #thanks\n",
    "#en_dic['aml'] = 1 #anti-money laundering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terms not in German Dictionary that should be\n",
    "de_dic['sowas'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = open('tweets.csv').read()\n",
    "tab_seperated = [item.split('\\t') for item in raw_text.split('\\n') if len(item.split('\\t')) >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ende_char = [\"á\",\"à\",\"ã\",\"ă\",\"â\",\"é\",\"è\",\"ê\",\"í\",\"ì\",\"ĩ\",\"ó\",\"ò\",\"õ\",\"ô\",\"ơ\",\"ú\",\"ù\",\"ũ\",\"ư\",\"ý\",\"ỳ\",\"đ\",\"ñ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = []\n",
    "for i in tab_seperated:\n",
    "    for char in non_ende_char:\n",
    "        if char in i[4]:\n",
    "            filtered.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120428"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tab_seperated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2449"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for i in range(len(tab_seperated)):\n",
    "    if tab_seperated[i][1] not in filtered:\n",
    "        data[tab_seperated[i][1]] = tab_seperated[i][4]\n",
    "    \n",
    "data_index = data.copy()    \n",
    "for tweet in data_index.keys():\n",
    "    data_index[tweet] = data_index[tweet].lower()\n",
    "    for i in range(len(contractions)):\n",
    "        if contractions[i] in data_index[tweet]:\n",
    "            data_index[tweet] = data_index[tweet].replace(contractions[i], fixes[i])\n",
    "    data_index[tweet] = re.sub('https?[^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub('[@#][^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[0-9][^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'\\w+\\.[^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[^a-zäöüß\\s]', ' ', data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[^\\w\\s]', ' ' , data_index[tweet])\n",
    "    \n",
    "terms = {}\n",
    "term_index = {}\n",
    "for num,tweet in data_index.items():\n",
    "    for word in tweet.split():\n",
    "        if word in term_index:\n",
    "            term_index[word].append(num)\n",
    "        elif word not in term_index:\n",
    "            term_index[word] = [num]\n",
    "        if word in terms:\n",
    "            terms[word] += 1\n",
    "        else:\n",
    "            terms[word] = 1\n",
    "\n",
    "for key in term_index.keys():\n",
    "    term_index[key] = sorted(term_index[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_language(term):\n",
    "        de_char = ['ä','ö','ü','ß']\n",
    "        de_score = 0\n",
    "        en_score = 0\n",
    "        for post in term_index[term]:\n",
    "            for i in data_index[post].strip().split():\n",
    "                for char in de_char:\n",
    "                    if char in i:\n",
    "                        de_score +=1\n",
    "                if i in stop_germ:\n",
    "                    de_score += 1\n",
    "                if i in stop_en:\n",
    "                    en_score += 1\n",
    "        if de_score>en_score:\n",
    "            return 'german'\n",
    "        elif de_score<en_score:\n",
    "            return 'english'\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_freq(dict):\n",
    "        freq = []\n",
    "        for term, val in dict.items():\n",
    "            freq.append((val, term))\n",
    "        freq = sorted(freq)[::-1]\n",
    "        freq_de = []\n",
    "        freq_en = []\n",
    "        for i,j in freq:\n",
    "            if is_language(j) == 'german':\n",
    "                freq_de.append(j)\n",
    "            else:\n",
    "                freq_en.append(j)\n",
    "        return freq_de, freq_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_de, top_en = top_freq(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misspells():\n",
    "        words =sorted([key for key in terms.keys()])\n",
    "        de = []\n",
    "        en = []\n",
    "        correct_en = []\n",
    "        correct_de = []\n",
    "        for word in words:\n",
    "            g_count = 0\n",
    "            e_count = 0\n",
    "            e_posts = []\n",
    "            g_posts = []\n",
    "            if word in en_dic:\n",
    "                correct_en.append(word)\n",
    "                continue\n",
    "            elif word in de_dic:\n",
    "                correct_de.append(word)\n",
    "                continue\n",
    "            #pos tags for lemmatization generation\n",
    "            tags = ['n','v','a','s','r']\n",
    "            en_lemma = {}\n",
    "            for tag in tags:\n",
    "                lemma = wnl.lemmatize(wnl,word=word, pos=tag) \n",
    "                en_lemma[lemma]= 1\n",
    "#             #German lemmatiziaton/stemming\n",
    "            de_lemma = word\n",
    "            \n",
    "            for post in term_index[word]:\n",
    "                if language(post) == 'german':\n",
    "                    ## get german misspelling count\n",
    "                    if de_lemma not in de_dic:\n",
    "                        if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                            g_count += 1\n",
    "                            g_posts.append(post)\n",
    "                        \n",
    "                elif language(post) == 'english':\n",
    "\n",
    "                    ## get german misspelling count\n",
    "                    if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                        e_count += 1\n",
    "                        e_posts.append(post)\n",
    "                        \n",
    "                else:\n",
    "                    lang = is_language(word)\n",
    "                    if lang == 'german':\n",
    "                        \n",
    "                        ## get german misspelling count    \n",
    "                        if de_lemma not in de_dic:\n",
    "                            if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                                g_count += 1\n",
    "                                g_posts.append(post)\n",
    "                    if lang == 'english':\n",
    "                        ## get english misspelling count\n",
    "                        if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                            e_count += 1\n",
    "                            e_posts.append(post)\n",
    "                            \n",
    "            if g_count > 0:\n",
    "                de.append((g_count, word, g_posts))\n",
    "            if e_count > 0:\n",
    "                en.append((e_count, word, e_posts))\n",
    "        return sorted(de)[::-1], sorted(en)[::-1], correct_de, correct_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    " def language(post):\n",
    "        de_char = ['ä','ö','ü','ß']\n",
    "        de_score = 0\n",
    "        en_score = 0\n",
    "        for i in data_index[post].split():\n",
    "            for char in de_char:\n",
    "                    if char in i:\n",
    "                        de_score +=1\n",
    "            if i in stop_germ or i in top_de[:200]:\n",
    "                de_score += 1\n",
    "            if i in stop_en or i in top_en[:200]:\n",
    "                en_score += 1\n",
    "        if de_score>en_score:\n",
    "            return 'german'\n",
    "        elif en_score>de_score:\n",
    "            return 'english'\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_mis, en_mis, terms_de, terms_en = get_misspells()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_damerau(word):\n",
    "    term = ''\n",
    "    for i in range(len(word)):\n",
    "        try:\n",
    "            if word[i] != word[i+2]:\n",
    "                term += word[i]\n",
    "        except: \n",
    "            term += word[i]\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    possible = {}\n",
    "    chunks = [(term[:i], term[i:])for i in range(len(term) + 1)]\n",
    "    for chunk1, chunk2 in chunks:\n",
    "        if chunk2:\n",
    "            possible[chunk1+chunk2[1:]] = 1\n",
    "            for char in alphabet:\n",
    "                possible[chunk1+char+chunk2[1:]] = 1\n",
    "        if len(chunk2) > 1:\n",
    "            possible[chunk1+chunk2[1]+chunk2[0]+chunk2[2:]] = 1\n",
    "        for char in alphabet:\n",
    "            possible[chunk1+char+chunk2] = 1\n",
    "    return possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_suggested(term):\n",
    "    \"\"\"Some terms such as the following can be force edited\n",
    "       They are slang to represent expressions\"\"\"\n",
    "#     if term == 'kinda':\n",
    "#         return \"kind of\"\n",
    "#     if term == 'gonna':\n",
    "#         return 'going to'\n",
    "#     if term == 'wanna':\n",
    "#         return 'want to'\n",
    "    suggestions = en_damerau(term)\n",
    "    suggested = []\n",
    "    for i in suggestions.keys():\n",
    "        tags = ['n','v','a','s','r']\n",
    "        lemmas = []\n",
    "        for tag in tags:\n",
    "            lemmas.append(wnl.lemmatize(wnl,word=i, pos=tag))\n",
    "        lemmas = set(lemmas)\n",
    "        for j in lemmas:\n",
    "            if j in en_dic:\n",
    "                suggested.append(i)\n",
    "    refined = [word for word in suggested if lev(term, word) == min(lev(term,word) for word in suggested)] \n",
    "    try:\n",
    "        best = sorted([(terms[word],word) for word in refined if word in top_en])[::-1]\n",
    "#         return best[0][1]\n",
    "        return [i for j,i in best][:3]\n",
    "    except:\n",
    "        return refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_damerau(word):\n",
    "    term = ''\n",
    "    for i in range(len(word)):\n",
    "        try:\n",
    "            if word[i] != word[i+2]:\n",
    "                term += word[i]\n",
    "        except: \n",
    "            term += word[i]\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyzäöüß\"\n",
    "    possible = {}\n",
    "    chunks = [(term[:i], term[i:])for i in range(len(term) + 1)]\n",
    "    for chunk1, chunk2 in chunks:\n",
    "        if chunk2:\n",
    "            possible[chunk1+chunk2[1:]] = 1\n",
    "            for char in alphabet:\n",
    "                possible[chunk1+char+chunk2[1:]] = 1\n",
    "        if len(chunk2) > 1:\n",
    "            possible[chunk1+chunk2[1]+chunk2[0]+chunk2[2:]] = 1\n",
    "        for char in alphabet:\n",
    "            possible[chunk1+char+chunk2] = 1\n",
    "    return possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_suggested(term):\n",
    "    suggestions = de_damerau(term)\n",
    "    suggested = []\n",
    "    for i in suggestions:\n",
    "        if i in de_dic:\n",
    "            suggested.append(i)\n",
    "    refined = [word for word in suggested if lev(term, word) == min(lev(term,word) for word in suggested)]\n",
    "    try:\n",
    "        best = sorted([(terms[word],word) for word in refined if word in de_dic and word in top_de])[::-1]\n",
    "        return [i for j,i in best][:3]\n",
    "    except:\n",
    "        return refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mis_en = []\n",
    "for count, word, posts in en_mis[:10]:\n",
    "    top_mis_en.append((word, count, en_suggested(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kinda', 2290, ['kinds', 'linda']),\n",
       " ('gonna', 492, ['donna', 'gonne']),\n",
       " ('wanna', 330, ['canna', 'hanna']),\n",
       " ('cuz', 93, ['cut', 'cup', 'coz']),\n",
       " ('lc', 87, ['bc', 'l', 'la']),\n",
       " ('gotta', 85, ['lotta']),\n",
       " ('bro', 85, ['bra', 'boo', 'brow']),\n",
       " ('ve', 74, ['me', 'be', 'he']),\n",
       " ('congrats', 71, []),\n",
       " ('rt', 70, ['it', 'at', 't'])]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_mis_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mis_de = []\n",
    "for count, word, posts in de_mis[:10]:\n",
    "    top_mis_de.append((word, count, de_suggested(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nen', 424, ['den', 'wen', 'ben']),\n",
       " ('nochmal', 294, ['nochmals']),\n",
       " ('nem', 233, ['dem', 'neu', 'net']),\n",
       " ('erstmal', 216, ['erstmals']),\n",
       " ('daß', 180, ['das', 'da', 'saß']),\n",
       " ('gibts', 164, ['gibt']),\n",
       " ('nich', 151, ['ich', 'nicht', 'noch']),\n",
       " ('zb', 147, ['zu', 'ob', 'ab']),\n",
       " ('vllt', 146, ['volt']),\n",
       " ('bir', 141, ['mir', 'wir', 'bin'])]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_mis_de"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
