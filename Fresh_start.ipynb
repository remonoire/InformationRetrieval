{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are all of our imports\n",
    "import re \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "from nltk.metrics.distance import edit_distance as lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contractions can be miss categorized as incorrect when you strip and filter the data\n",
    "#fixes are uncontracted contractions\n",
    "#stop_en and stop_germ are the stopwords\n",
    "contractions = [\"´\",\"‘\",\"’\",\"it's\", \"he's\",\"she's\",\"that's\", \"what's\", \"there's\", \"aren't\",\\\n",
    "            \"[newline]\", \"'m\", \"'ve\",\"n't\", \"'ll\",\"'re\", \"won't\", \"'d\",\"geht's\",\"gibt's\", \"'s\", ' xd']\n",
    "fixes = [\"'\",\"'\",\"'\",\"it is\", \"he is\",\"she is\",\"that is\", \"what is\", \"there is\",\"are not\",\\\n",
    "                 \" \", \" am\", \" have\", \" not\", \" will\", \" are\", \"will not\", \" would\",\"geht es\",\"gibt es\", \"\", ' ']\n",
    "stop_en = set(stopwords.words('english'))\n",
    "stop_germ = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening our german and english dictionairies\n",
    "de_dic = {}\n",
    "with open('german.dic', 'r', encoding='latin-1') as f:\n",
    "    for row in f:\n",
    "        if len(row) >1:\n",
    "            de_dic[row.strip().lower()] = 1\n",
    "en_dic = {}\n",
    "with open('english.dic', 'r',) as f:\n",
    "    for row in f:\n",
    "        en_dic[row.strip().lower()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words that are not in the english dictionary but should be- if we don't do this then these words will be erroneosly \n",
    "#labeled as misspells\n",
    "en_dic['anymore'] = 1\n",
    "en_dic['adhd'] = 1\n",
    "en_dic['asshole'] = 1\n",
    "en_dic['fucking'] = 1\n",
    "en_dic['porn'] = 1\n",
    "en_dic['fuck'] = 1\n",
    "en_dic['proud'] = 1\n",
    "en_dic['others'] = 1\n",
    "en_dic['mom'] = 1\n",
    "en_dic['ptsd'] = 1\n",
    "en_dic['europe']= 1\n",
    "en_dic['tumour'] = 1\n",
    "en_dic['tumours'] = 1\n",
    "en_dic['stats'] = 1\n",
    "en_dic['favourite'] = 1\n",
    "en_dic['boyfriend'] = 1\n",
    "en_dic['fortnite'] = 1 #game name\n",
    "en_dic['bts'] = 1 # Korean pop group\n",
    "\n",
    "#we considered also doing this for acronyms because they are not accidental misspells they are just not in our dictionary \n",
    "#but utimately for the sake of the assignment and the dictionary given to us we decided not to input them into our dictionary\n",
    "# en_dic['lol'] = 1 #laugh out loud\n",
    "# en_dic['omg'] = 1 #oh my god\n",
    "# en_dic['af'] = 1 #as fuck\n",
    "# en_dic['tbh'] = 1 #to be honest\n",
    "# en_dic['bc'] = 1 #because\n",
    "# en_dic['idk'] = 1 #i don't know\n",
    "# en_dic['rn'] = 1 #right now\n",
    "# en_dic['ppl'] = 1 #people\n",
    "# en_dic['lmao'] = 1 #laugh my ass off\n",
    "# en_dic['wtf'] = 1 #what the fuck\n",
    "# en_dic['btw'] = 1 #by the way\n",
    "# en_dic['pls'] = 1 #please\n",
    "# en_dic['thx'] = 1 #thanks\n",
    "#en_dic['aml'] = 1 #anti-money laundering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terms not in German Dictionary that should be, we are all native English speakers\n",
    "#so there could be more than just this\n",
    "de_dic['sowas'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init___(self, data=None):\n",
    "        self.head = data\n",
    "        self.next = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to open and read our tweets to begin the preprocessing step\n",
    "raw_text = open('tweets.csv').read()\n",
    "tab_seperated = [item.split('\\t') for item in raw_text.split('\\n') if len(item.split('\\t')) >= 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-English or German characters, used to filter out foreign tweets\n",
    "non_ende_char = [\"á\",\"à\",\"ã\",\"ă\",\"â\",\"é\",\"è\",\"ê\",\"í\",\"ì\",\"ĩ\",\"ó\",\"ò\",\"õ\",\"ô\",\"ơ\",\"ú\",\"ù\",\"ũ\",\"ư\",\"ý\",\"ỳ\",\"đ\",\"ñ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building our tweet filter\n",
    "filtered = []\n",
    "for i in tab_seperated:\n",
    "    for char in non_ende_char:\n",
    "        if char in i[4]:\n",
    "            filtered.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use regex to remove any websites, numbers, tagged twitter users, hashtags, \n",
    "#and basically anything that is not in the English and German alphabets\n",
    "data = {}\n",
    "for i in range(len(tab_seperated)):\n",
    "    #extracting the filtered tweets and IDs\n",
    "    if tab_seperated[i][1] not in filtered:\n",
    "        data[tab_seperated[i][1]] = tab_seperated[i][4]\n",
    "\n",
    "#Preprocessing and breaking apart common contractions\n",
    "data_index = data.copy()    \n",
    "for tweet in data_index.keys():\n",
    "    data_index[tweet] = data_index[tweet].lower()\n",
    "    for i in range(len(contractions)):\n",
    "        if contractions[i] in data_index[tweet]:\n",
    "            data_index[tweet] = data_index[tweet].replace(contractions[i], fixes[i])\n",
    "    data_index[tweet] = re.sub('https?[^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub('[@#][^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[0-9][^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'\\w+\\.[^\\s]+', ' ' , data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[^a-zäöüß\\s]', ' ', data_index[tweet])\n",
    "    data_index[tweet] = re.sub(r'[^\\w\\s]', ' ' , data_index[tweet])\n",
    "\n",
    "#Here we are building our dict of terms and frequenceis, as well as a dict with sorted docIDs\n",
    "terms = {}\n",
    "term_index = {}\n",
    "for num,tweet in data_index.items():\n",
    "    for word in tweet.split():\n",
    "        if word in term_index:\n",
    "            term_index[word].append(num)\n",
    "        elif word not in term_index:\n",
    "            term_index[word] = [num]\n",
    "        if word in terms:\n",
    "            terms[word] += 1\n",
    "        else:\n",
    "            terms[word] = 1\n",
    "sorted_dict={}\n",
    "x=sorted(list(term_index.keys()))\n",
    "for i in x:\n",
    "    n = Node()\n",
    "    n.next = term_index[i]\n",
    "    sorted_dict[i]= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['967181845652787202',\n",
       " '976244121512890369',\n",
       " '989537674477342725',\n",
       " '998605522927484928',\n",
       " '993978998190477312',\n",
       " '1011372495658782725',\n",
       " '1005588393831563266',\n",
       " '1002315223053762560']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_dict['aaa'].next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function determines if a word is German or English \n",
    "#Given on number of tweets within the postings list based on stop words and foreign characters\n",
    "def is_language(term):\n",
    "        de_char = ['ä','ö','ü','ß']\n",
    "        de_score = 0\n",
    "        en_score = 0\n",
    "        for post in term_index[term]:\n",
    "            for i in data_index[post].strip().split():\n",
    "                for char in de_char:\n",
    "                    if char in i:\n",
    "                        de_score +=1\n",
    "                if i in stop_germ:\n",
    "                    de_score += 1\n",
    "                if i in stop_en:\n",
    "                    en_score += 1\n",
    "        if de_score>en_score:\n",
    "            return 'german'\n",
    "        elif de_score<en_score:\n",
    "            return 'english'\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to find which words are the most frequent in there respective langauges\n",
    "def top_freq(dict):\n",
    "        freq = []\n",
    "        for term, val in dict.items():\n",
    "            freq.append((val, term))\n",
    "        freq = sorted(freq)[::-1]\n",
    "        freq_de = []\n",
    "        freq_en = []\n",
    "        for i,j in freq:\n",
    "            if is_language(j) == 'german':\n",
    "                freq_de.append(j)\n",
    "            else:\n",
    "                freq_en.append(j)\n",
    "        return freq_de, freq_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generated the english and german terms, sorted by descending frequency\n",
    "top_de, top_en = top_freq(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to find out if the tweet is English or German \n",
    "#Here we use a refined search to better identify if a tweet is english or Germn\n",
    "#given stop words and our most frequent words occuring in English or German\n",
    "#If it fails to identify, it will return none\n",
    "def language(post):\n",
    "        de_char = ['ä','ö','ü','ß']\n",
    "        de_score = 0\n",
    "        en_score = 0\n",
    "        for i in data_index[post].split():\n",
    "            for char in de_char:\n",
    "                    if char in i:\n",
    "                        de_score +=1\n",
    "            if i in stop_germ or i in top_de[:200]:\n",
    "                de_score += 1\n",
    "            if i in stop_en or i in top_en[:200]:\n",
    "                en_score += 1\n",
    "        if de_score>en_score:\n",
    "            return 'german'\n",
    "        elif en_score>de_score:\n",
    "            return 'english'\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function finds misspelled words\n",
    "def get_misspells():\n",
    "        #Here we generate a list of terms and list to partition them in\n",
    "        words =sorted([key for key in terms.keys()])\n",
    "        de = []\n",
    "        en = []\n",
    "        correct_en = []\n",
    "        correct_de = []\n",
    "        for word in words:\n",
    "            g_count = 0\n",
    "            e_count = 0\n",
    "            e_posts = []\n",
    "            g_posts = []\n",
    "            #here we sort correct words separate correct_addicitonal terms\n",
    "            if word in en_dic:\n",
    "                correct_en.append(word)\n",
    "                continue\n",
    "            elif word in de_dic:\n",
    "                correct_de.append(word)\n",
    "                continue\n",
    "            #pos tags for lemmatization generation\n",
    "            tags = ['n','v','a','s','r']\n",
    "            en_lemma = {}\n",
    "            for tag in tags:\n",
    "                lemma = wnl.lemmatize(wnl,word=word, pos=tag) \n",
    "                en_lemma[lemma]= 1\n",
    "#             #German lemmatiziaton/stemming\n",
    "            de_lemma = word\n",
    "            \n",
    "        #here we search post by post to determin if term in index\n",
    "        # is German or English, to see if in the context it is a misspell\n",
    "            for post in term_index[word]:\n",
    "                if language(post) == 'german':\n",
    "                    ## get german misspelling count\n",
    "                    if de_lemma not in de_dic:\n",
    "                        if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                            g_count += 1\n",
    "                            g_posts.append(post)\n",
    "                    else:\n",
    "                        correct_de.append(word)\n",
    "                        \n",
    "                elif language(post) == 'english':\n",
    "\n",
    "                    ## get german misspelling count\n",
    "                    if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                        e_count += 1\n",
    "                        e_posts.append(post)\n",
    "                    else:\n",
    "                        correct_en.append(word)\n",
    "                \n",
    "            #Here if the refined methon is not able to solve,\n",
    "            #It falls back to the likelihood of based on occurance in German or English Tweets \n",
    "                else:\n",
    "                    lang = is_language(word)\n",
    "                    if lang == 'german':\n",
    "                        \n",
    "                        ## get german misspelling count    \n",
    "                        if de_lemma not in de_dic:\n",
    "                            if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                                g_count += 1\n",
    "                                g_posts.append(post)\n",
    "                        else:\n",
    "                            correct_de.append(word)\n",
    "                    if lang == 'english':\n",
    "                        ## get english misspelling count\n",
    "                        if len([i for i in en_lemma.keys() if i in en_dic]) == 0:\n",
    "                            e_count += 1\n",
    "                            e_posts.append(post)\n",
    "                        else:\n",
    "                            correct_en.append(word)\n",
    "                            \n",
    "            if g_count > 0:\n",
    "                de.append((g_count, word, g_posts))\n",
    "            if e_count > 0:\n",
    "                en.append((e_count, word, e_posts))\n",
    "                \n",
    "        #retruning list of misspells and correct lists\n",
    "        return sorted(de)[::-1], sorted(en)[::-1], correct_de, correct_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we generate our lists of misspells\n",
    "de_mis, en_mis, terms_de, terms_en = get_misspells()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function calculates the Damerau distance of the misspelled words that were found in the function above\n",
    "def en_damerau(word):\n",
    "    #here we reduce the workload by reducing same characters \n",
    "    #that occur more than twice in a row to just two characters to increase accuracy \n",
    "    term = ''\n",
    "    for i in range(len(word)):\n",
    "        try:\n",
    "            if word[i] != word[i+2]:\n",
    "                term += word[i]\n",
    "        except: \n",
    "            term += word[i]\n",
    "    \n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    possible = {}\n",
    "    \n",
    "    #we are genearting a dictionary of terms that are 1 damerau distance from the term\n",
    "    chunks = [(term[:i], term[i:])for i in range(len(term) + 1)]\n",
    "    for chunk1, chunk2 in chunks:\n",
    "        if chunk2:\n",
    "            #subtraction\n",
    "            possible[chunk1+chunk2[1:]] = 1\n",
    "            for char in alphabet:\n",
    "                #substitution\n",
    "                possible[chunk1+char+chunk2[1:]] = 1\n",
    "        if len(chunk2) > 1:\n",
    "            #transposition\n",
    "            possible[chunk1+chunk2[1]+chunk2[0]+chunk2[2:]] = 1\n",
    "        for char in alphabet:\n",
    "            #addition\n",
    "            possible[chunk1+char+chunk2] = 1\n",
    "    return possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function finds the suggested terms based on the damerau distance for English words\n",
    "def en_suggested(term):\n",
    "    \"\"\"Some terms such as the following can be force edited\n",
    "       They are slang to represent expressions\"\"\"\n",
    "#     if term == 'kinda':\n",
    "#         return \"kind of\"\n",
    "#     if term == 'gonna':\n",
    "#         return 'going to'\n",
    "#     if term == 'wanna':\n",
    "#         return 'want to'\n",
    "    suggestions = en_damerau(term)\n",
    "    suggested = []\n",
    "    #Lemmatizing the suggestions to get a more accurate reference\n",
    "    for i in suggestions.keys():\n",
    "        tags = ['n','v','a','s','r']\n",
    "        lemmas = []\n",
    "        for tag in tags:\n",
    "            lemmas.append(wnl.lemmatize(wnl,word=i, pos=tag))\n",
    "        lemmas = set(lemmas)\n",
    "        for j in lemmas:\n",
    "            if j in en_dic:\n",
    "                suggested.append(i)\n",
    "    #Refining the words that were suggested using edited distance again\n",
    "    refined = [word for word in suggested if lev(term, word) == min(lev(term,word) for word in suggested)] \n",
    "    try:\n",
    "        #further refining the search\n",
    "        best = sorted([(terms[word],word) for word in refined if word in terms_en])[::-1]\n",
    "        return [i for j,i in best][:3]\n",
    "    except:\n",
    "        return refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function finds the damerau distance for the misspelled German words\n",
    "\n",
    "def de_damerau(word):\n",
    "    #here we reduce the workload by reducing same characters \n",
    "    #that occur more than twice in a row to just two characters to increase accuracy \n",
    "    term = ''\n",
    "    for i in range(len(word)):\n",
    "        try:\n",
    "            if word[i] != word[i+2]:\n",
    "                term += word[i]\n",
    "        except: \n",
    "            term += word[i]\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyzäöüß\"\n",
    "    possible = {}\n",
    "    #we are genearting a dictionary of terms that are 1 damerau distance from the term\n",
    "    chunks = [(term[:i], term[i:])for i in range(len(term) + 1)]\n",
    "    for chunk1, chunk2 in chunks:\n",
    "        if chunk2:\n",
    "            #subtraction\n",
    "            possible[chunk1+chunk2[1:]] = 1\n",
    "            for char in alphabet:\n",
    "                #substitution\n",
    "                possible[chunk1+char+chunk2[1:]] = 1\n",
    "        if len(chunk2) > 1:\n",
    "            #transposition\n",
    "            possible[chunk1+chunk2[1]+chunk2[0]+chunk2[2:]] = 1\n",
    "        for char in alphabet:\n",
    "            #addition\n",
    "            possible[chunk1+char+chunk2] = 1\n",
    "    return possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This function finds the suggested terms based on the damerau distance for German words\n",
    "\n",
    "def de_suggested(term):\n",
    "    suggestions = de_damerau(term)\n",
    "    suggested = []\n",
    "    for i in suggestions:\n",
    "        if i in de_dic:\n",
    "            suggested.append(i)\n",
    "    #Refining the words that were suggested using edited distance again\n",
    "    refined = [word for word in suggested if lev(term, word) == min(lev(term,word) for word in suggested)]\n",
    "    try:\n",
    "        #further refining the search\n",
    "        best = sorted([(terms[word],word) for word in refined if word in de_dic and word in terms_de])[::-1]\n",
    "        return [i for j,i in best][:3]\n",
    "    except:\n",
    "        return refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This gives us the top misspelled English words\n",
    "top_mis_en = []\n",
    "for count, word, posts in en_mis[:10]:\n",
    "    top_mis_en.append((word, count, en_suggested(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kinda', 2290, ['kind', 'linda']),\n",
       " ('bc', 712, ['be', 'by', 'b']),\n",
       " ('gonna', 492, ['donna', 'gonne', 'gona']),\n",
       " ('lol', 395, ['vol', 'lo', 'pol']),\n",
       " ('omg', 377, ['om', 'og']),\n",
       " ('wanna', 330, ['anna', 'canna', 'hanna']),\n",
       " ('rn', 297, ['in', 'an', 'on']),\n",
       " ('tbh', 284, ['th', 'tch']),\n",
       " ('idk', 261, ['id', 'ink', 'ilk']),\n",
       " ('ppl', 217, ['pol', 'pal'])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it should be noted that even though some of the outputted words were not intentional misspells they were not in our dictionary \n",
    "#so they were considered misspells we ultimately decided to leave them out in order to comply to the constraints \n",
    "#of the dictionarys that were given to us in the assignment\n",
    "top_mis_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This gives us the top misspelled German words\n",
    "top_mis_de = []\n",
    "for count, word, posts in de_mis[:10]:\n",
    "    top_mis_de.append((word, count, de_suggested(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nen', 424, ['ren', 'nn']),\n",
       " ('nochmal', 294, ['nochmals']),\n",
       " ('nem', 233, ['dem', 'neu', 'nm']),\n",
       " ('erstmal', 216, ['erstmals']),\n",
       " ('lol', 214, ['hol', 'mol', 'aol']),\n",
       " ('daß', 180, ['saß', 'maß', 'dax']),\n",
       " ('gibts', 164, ['gibt', 'gifts']),\n",
       " ('nich', 151, ['nicht', 'noch', 'sich']),\n",
       " ('zb', 147, ['zu', 'ob', 'tb']),\n",
       " ('vllt', 146, [])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 10 German misspells\n",
    "top_mis_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
